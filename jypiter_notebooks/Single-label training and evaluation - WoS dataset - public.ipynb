{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dict=set(stopwords.words(\"english-v2-uspto-sklearn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944ebbf",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b922e9d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_data(part, trainDF):\n",
    "\n",
    "#This function presesses the text and the labels that have been loaded in a dataframe\n",
    "\n",
    "# part: the textual field that we will use \n",
    "# trainDF: the dataframe\n",
    "\n",
    "    #text\n",
    "    if part==1:\n",
    "        trainDF=trainDF.rename(columns={'keywords': 'text'})\n",
    "    elif part==2:\n",
    "        trainDF=trainDF.rename(columns={'Abstract': 'text'})\n",
    "\n",
    "    #labels    \n",
    "    trainDF=trainDF.rename(columns={'Y': 'label'})\n",
    "   \n",
    "    #replace the na rows with \"\" otherwise it returns an error\n",
    "    trainDF['text']=trainDF['text'].fillna(\"\")\n",
    "    #lowercase\n",
    "    trainDF['text']=trainDF['text'].str.lower()\n",
    "    #delete all symbols except for a-z\n",
    "    trainDF['text']=trainDF['text'].replace('[^a-z]', ' ', regex=True)       \n",
    "    #delete the stopwords\n",
    "    trainDF['text']=trainDF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_dict)]))\n",
    "    #keep the first #number of words\n",
    "    trainDF['text']=trainDF['text'].str.split().str[0:number_of_words]\n",
    "    trainDF['text']=trainDF['text'].str.join(' ')\n",
    "    \n",
    "    print(\"The data has been loaded! \\n\")\n",
    "    \n",
    "    return trainDF\n",
    "\n",
    "def encode_labels(trainDF):\n",
    "\n",
    "#This function encodes the labels with OneHotEncoder\n",
    "\n",
    "# trainDF: the dataframe that we will use \n",
    "\n",
    "    labels_val=trainDF['label'].values\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(labels_val.reshape(-1, 1))\n",
    "    print(\"The labels have been encoded! \\n\")\n",
    "\n",
    "    return onehot_encoder, onehot_encoded\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of labels: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def split_dataset(trainDF, onehot_encoded):\n",
    "\n",
    "#This function splits the dataset into training and rtesting set\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(trainDF['text'], onehot_encoded, test_size=0.1, random_state=42) #stratify=onehot_encoded\n",
    "    print(\"The dataset has been splitted into training and testing set! \\n\")\n",
    "    \n",
    "    return train_x, train_y,  test_x, test_y\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "\n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "\n",
    "    maxlen=number_of_words\n",
    "\n",
    "    seq_x = sequence.pad_sequences(token.texts_to_sequences(x), maxlen)\n",
    "    print(\"The text has been converted to tokens! \\n\")\n",
    "\n",
    "    return seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    embeddings_dict = {}\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    print(\"The word embeddings has been loaded! \\n\")\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index, x):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "\n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, x))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)         \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def create_bidirectional_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "# This function creates the classification model based on Bi-LSTM and categorical loss\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "    # Add the Embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    # Add the SpatialDropout1D Layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "    # Add a Bidirectional Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)                                                                                        \n",
    "    # Add the Output Layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "                                                                                            \n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "\n",
    "# This function makes the predictions in the test data\n",
    "\n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('The predictions on test data have been calculated! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true                                                                                           \n",
    "\n",
    "def calculate_metrics(predictions, prediction, y_true, number_of_test_data):\n",
    "\n",
    "# This function calculates for single-label classification the accuracy, MRR, the accuracy/precision @3, @5 and @10\n",
    "    \n",
    "    #Accuracy \n",
    "    accuracy_total=metrics.accuracy_score(prediction, y_true)*100\n",
    "    print(\"Accuracy:\", accuracy_total)\n",
    "\n",
    "    #MRR, P@3, P@5, P@10\n",
    "    all_rr=[]\n",
    "    number_of_top_three=0\n",
    "    number_of_top_five=0\n",
    "    number_of_top_ten=0\n",
    "\n",
    "    predictions_2=predictions.argsort()\n",
    "    predictions_3=np.fliplr(predictions_2)\n",
    "    for i in range (0, number_of_test_data):\n",
    "        specific_prediction=predictions_3[i,:]\n",
    "        list1 = specific_prediction.tolist()\n",
    "        target=y_true[i]\n",
    "        prediction_rank=list1.index(target)+1 \n",
    "        #MRR\n",
    "        RR=1/prediction_rank\n",
    "        all_rr.append(RR)\n",
    "        #P@3\n",
    "        if prediction_rank<= 3:\n",
    "            number_of_top_three=number_of_top_three+1\n",
    "        #P@5\n",
    "        if prediction_rank<= 5:\n",
    "            number_of_top_five=number_of_top_five+1     \n",
    "        #P@10\n",
    "        if prediction_rank<= 10:\n",
    "            number_of_top_ten=number_of_top_ten+1 \n",
    "    MRR=np.mean(all_rr)\n",
    "    print(\"MRR:\", MRR)\n",
    "    P3=number_of_top_three/number_of_test_data*100\n",
    "    print(\"P@3:\", P3)\n",
    "    P5=number_of_top_five/number_of_test_data*100\n",
    "    print(\"P@5:\", P5)\n",
    "    P10=number_of_top_ten/number_of_test_data*100\n",
    "    print(\"P@10:\", P10)\n",
    "    \n",
    "    return accuracy_total, MRR, P3, P5, P10\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ee233",
   "metadata": {},
   "source": [
    "### Load the dataset and run the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a690d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "DF shape: (46985, 7)\n",
      "1\n",
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  134 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 38976 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 180, 300)          11693100  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 134)               26934     \n",
      "=================================================================\n",
      "Total params: 12,040,834\n",
      "Trainable params: 347,734\n",
      "Non-trainable params: 11,693,100\n",
      "_________________________________________________________________\n",
      "Train on 42286 samples\n",
      "42286/42286 [==============================] - 105s 2ms/sample - loss: 4.8448 - acc: 0.0145\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 1.447116407746329\n",
      "MRR: 0.061853675881861525\n",
      "P@3: 4.724409448818897\n",
      "P@5: 7.29942540966163\n",
      "P@10: 13.215577782506916\n",
      "DF shape: (46985, 7)\n",
      "2\n",
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  134 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 105850 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 180, 300)          31755300  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 134)               26934     \n",
      "=================================================================\n",
      "Total params: 32,103,034\n",
      "Trainable params: 347,734\n",
      "Non-trainable params: 31,755,300\n",
      "_________________________________________________________________\n",
      "Train on 42286 samples\n",
      "42286/42286 [==============================] - 106s 3ms/sample - loss: 4.8440 - acc: 0.0142\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 1.447116407746329\n",
      "MRR: 0.062349517820700445\n",
      "P@3: 4.724409448818897\n",
      "P@5: 7.29942540966163\n",
      "P@10: 13.194296658863589\n",
      "2\n",
      "DF shape: (11967, 7)\n",
      "1\n",
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  35 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 17636 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 180, 300)          5291100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 35)                7035      \n",
      "=================================================================\n",
      "Total params: 5,618,935\n",
      "Trainable params: 327,835\n",
      "Non-trainable params: 5,291,100\n",
      "_________________________________________________________________\n",
      "Train on 10770 samples\n",
      "10770/10770 [==============================] - 28s 3ms/sample - loss: 3.5315 - acc: 0.0564\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 5.597326649958228\n",
      "MRR: 0.15173463527323208\n",
      "P@3: 12.280701754385964\n",
      "P@5: 18.128654970760234\n",
      "P@10: 35.00417710944026\n",
      "DF shape: (11967, 7)\n",
      "2\n",
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  35 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 51480 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 180, 300)          15444300  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 35)                7035      \n",
      "=================================================================\n",
      "Total params: 15,772,135\n",
      "Trainable params: 327,835\n",
      "Non-trainable params: 15,444,300\n",
      "_________________________________________________________________\n",
      "Train on 10770 samples\n",
      "10770/10770 [==============================] - 28s 3ms/sample - loss: 3.5338 - acc: 0.0569\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 5.597326649958228\n",
      "MRR: 0.15266192412746993\n",
      "P@3: 12.447786131996658\n",
      "P@5: 18.9640768588137\n",
      "P@10: 34.41938178780284\n",
      "3\n",
      "DF shape: (5736, 7)\n",
      "1\n",
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  11 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 12534 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 180, 300)          3760500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                2211      \n",
      "=================================================================\n",
      "Total params: 4,083,511\n",
      "Trainable params: 323,011\n",
      "Non-trainable params: 3,760,500\n",
      "_________________________________________________________________\n",
      "Train on 5162 samples\n",
      "5162/5162 [==============================] - 14s 3ms/sample - loss: 2.3765 - acc: 0.1246\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 13.588850174216027\n",
      "MRR: 0.340839454776737\n",
      "P@3: 41.46341463414634\n",
      "P@5: 57.14285714285714\n",
      "P@10: 93.37979094076655\n",
      "DF shape: (5736, 7)\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  11 \n",
      "\n",
      "The dataset has been splitted into training and testing set! \n",
      "\n",
      "Number of unique words: 36672 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 180)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 180, 300)          11001900  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 180, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                2211      \n",
      "=================================================================\n",
      "Total params: 11,324,911\n",
      "Trainable params: 323,011\n",
      "Non-trainable params: 11,001,900\n",
      "_________________________________________________________________\n",
      "Train on 5162 samples\n",
      "5162/5162 [==============================] - 15s 3ms/sample - loss: 2.3710 - acc: 0.1213\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "Accuracy: 13.937282229965156\n",
      "MRR: 0.3424315956981462\n",
      "P@3: 41.46341463414634\n",
      "P@5: 57.14285714285714\n",
      "P@10: 93.37979094076655\n"
     ]
    }
   ],
   "source": [
    "#The below instalations are necessary to read the excel file of the dataset\n",
    "#!pip install openpyxl \n",
    "#!pip install xlrd \n",
    "\n",
    "number_of_words = 180\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "datasets=[1, 2, 3] #1: WOS46985, 2: WOS11967, 3: WOS5736 \n",
    "parts=[1, 2] #1: keywords, 2: Abstract\n",
    "\n",
    "for j, dataset in enumerate(datasets):\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    file=\"/WebOfScience/Meta-data/Data.xlsx\"\n",
    "    initial_DF=pd.read_excel(file, sheet_name='abstracts', engine='openpyxl')\n",
    "\n",
    "    if dataset == 2 or dataset == 3: \n",
    "        if dataset == 2: \n",
    "            file_part=\"/WebOfScience/WOS11967/X.txt\"    \n",
    "        if dataset == 3: \n",
    "            file_part=\"/WebOfScience/WOS5736/X.txt\"    \n",
    "\n",
    "        DF_part = pd.read_csv(file_part, header=None, sep=\"\\t\")\n",
    "        DF_part=DF_part.rename(columns={0: 'Abstract'})\n",
    "\n",
    "        DF = pd.merge(DF_part, initial_DF, how='left', on='Abstract')\n",
    "    else:\n",
    "        DF=initial_DF.copy()\n",
    "\n",
    "    for i, part in enumerate(parts):\n",
    "        \n",
    "        print(\"DF shape:\", DF.shape)       \n",
    "        print(part)\n",
    "        \n",
    "        DF_processed=process_data(part, DF)\n",
    "\n",
    "        onehot_encoder, onehot_encoded=encode_labels(DF_processed)\n",
    "        number_of_codes=enumarate_codes(onehot_encoded)\n",
    "        train_x_p1, train_y_p1, test_x_p1, test_y_p1 =split_dataset(DF_processed, onehot_encoded)\n",
    "        token_p1, word_index_p1=tokenize_text(DF_processed)\n",
    "\n",
    "        train_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1)    \n",
    "        test_seq_x_p1 =convert_text(number_of_words, token_p1, test_x_p1)    \n",
    "\n",
    "        embeddings_index = load_language_model('/embeddings/glove.6B.300d.txt')\n",
    "        embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1, 300)\n",
    "\n",
    "        kill_model()\n",
    "        classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        history1=classifier1.fit(train_seq_x_p1, train_y_p1, epochs=epochs, batch_size=batch_size, verbose=1)           \n",
    "\n",
    "        #Save the trained classifier\n",
    "        #classifier1.save(\"wos_11967_part\"+str(part)+\"_40epoch\")\n",
    "\n",
    "        number_of_test_data=np.shape(test_y_p1)\n",
    "        number_of_test_data=number_of_test_data[0]\n",
    "\n",
    "        predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_p1, classifier1)            \n",
    "        accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics(predictions_p1, prediction_p1, y_true1, number_of_test_data)\n",
    "\n",
    "        #Save the final predictions\n",
    "        #df=pd.DataFrame(predictions_p1)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #file_name=\"wos_\"+str(dataset)+\"_part\"+str(part)+\"_40epoch.csv\"\n",
    "        #df.to_csv(file_name, header=False, index=False)\n",
    "        \n",
    "    #Save qrel\n",
    "    #df=pd.DataFrame(y_true1)\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"wos_+str(dataset)+_rel_numbers.csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db09174",
   "metadata": {},
   "source": [
    "### Load the stored predictions and qrel and create the ensemble of above classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for the other two datasets\n",
    "in1=pd.read_csv('wos_11967_part1_40epoch.csv',header=None)\n",
    "in2=pd.read_csv('wos_11967_part2_40epoch.csv',header=None)\n",
    "\n",
    "in1_predictions=in1.to_numpy()\n",
    "in2_predictions=in2.to_numpy()\n",
    "\n",
    "in1_prediction = np.argmax(in1_predictions, axis = -1) \n",
    "in2_prediction = np.argmax(in2_predictions, axis = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv('wos_11967_rel_numbers.csv',header=None)\n",
    "true=true.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d66568",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_data_p1=in1_prediction.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions2(predictions_p1, predictions_p2, number_of_test_data_p1, en):\n",
    "\n",
    "#This functions calculates the averaging of predictions for each label\n",
    "    \n",
    "    average_predictions = []\n",
    "    i=0\n",
    "    \n",
    "    for i in range(number_of_test_data_p1):\n",
    "        a=np.mean([predictions_p1[i], predictions_p2[i]], axis=0)\n",
    "        average_predictions.append(a)\n",
    "    \n",
    "    average_predictions_2 = np.array(average_predictions)      \n",
    "    \n",
    "    average_prediction = np.argmax(average_predictions, axis = -1) \n",
    "    print(type(average_predictions_2), type(average_prediction))\n",
    "\n",
    "    print('The ensemble predictions have been calculated! \\n')\n",
    "\n",
    "    return average_predictions_2, average_prediction\n",
    "\n",
    "ensembles=2 #the number of base classifiers combined using an avaraging function\n",
    "average_predictions, average_prediction=ensemble_predictions2(in1_predictions, in2_predictions, number_of_test_data_p1, ensembles)\n",
    "accuracy_total, MRR, P3, P5, P10 = calculate_metrics(average_predictions, average_prediction, true, number_of_test_data_p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
