{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d11b50",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents(file1):\n",
    "    \n",
    "# This function loads the data\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[0,1])\n",
    "    trainDF=trainDF.rename(columns={0: 'label'})\n",
    "    trainDF=trainDF.rename(columns={1: 'text'})\n",
    "\n",
    "    return trainDF\n",
    "\n",
    "def encode_labels(trainDF):\n",
    "    \n",
    "#This function encodes the labels with OneHotEncoder\n",
    "\n",
    "    labels_val=trainDF['label'].values\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(labels_val.reshape(-1, 1))\n",
    "    \n",
    "    return onehot_encoded, onehot_encoder\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of codes: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def split_dataset(trainDF, onehot_encoded):\n",
    "\n",
    "# This function splits the data into train, validation and test set (80:10:10)\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], onehot_encoded, test_size=0.2, random_state=42) #stratify=onehot_encoded\n",
    "    test_x, valid_x, test_y, valid_y = train_test_split(valid_x, valid_y, test_size=0.5, random_state=41)\n",
    "        \n",
    "    #Number of data per split\n",
    "    \n",
    "    number_of_train_data=np.shape(train_x)\n",
    "    number_of_train_data=number_of_train_data[0]\n",
    "    print(\"Number of train data:\", number_of_train_data)\n",
    "\n",
    "    number_of_valid_data=np.shape(valid_x)\n",
    "    number_of_valid_data=number_of_valid_data[0]\n",
    "    print(\"Number of validation data:\",number_of_valid_data)\n",
    "\n",
    "    number_of_test_data=np.shape(test_x)\n",
    "    number_of_test_data=number_of_test_data[0]\n",
    "    print(\"Number of test data:\",number_of_test_data, \"\\n\")\n",
    "    \n",
    "    return train_x, train_y,  valid_x, valid_y, test_x, test_y, number_of_test_data\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "        \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, train_x, valid_x, test_x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "    \n",
    "    maxlen=number_of_words\n",
    "\n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen)\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen)\n",
    "    test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen)\n",
    "    print('convert text to tokens - Done! \\n')\n",
    "\n",
    "    return train_seq_x, valid_seq_x, test_seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    \n",
    "    print(\"load_patentVec-Done! \\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "    \n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def create_bidirectional_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    \n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2f44b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "def train(x_train, y_train, x_test, y_test, model, batch_size, epochs):\n",
    "    history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            shuffle=True)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    # print('Test loss:', scores[0])\n",
    "    # print('Test accuracy:', scores[1])\n",
    "    return scores\n",
    "\n",
    "def predict(model, x_test):\n",
    "    test_classes = model.predict(x_test, verbose=0)\n",
    "    test_classes = np.argmax(test_classes, axis=1)\n",
    "    # print(test_classes.shape)\n",
    "    return test_classes\n",
    "\n",
    "def mostcommon(array):\n",
    "    '''return the most common value of an array'''\n",
    "    return np.bincount(array).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_train_model(n_learners, epochs_lst, batch_size, random=True, number_of_codes=731):\n",
    "\n",
    "#This function creates the base classifiers and calculates their predictions\n",
    "#Then, the function creates the adaboost ensemble classifier by keeping the probabilities from each base classifier\n",
    "#and calculating the final predictions based on alphas and predictions from base classifiers\n",
    "    \n",
    "    num_classes = number_of_codes\n",
    "    K = float(num_classes)\n",
    "    \n",
    "    #transform one-hot encoded codes to code numbers\n",
    "    train_y_p1_help=np.argmax(train_y_p1,axis = -1)\n",
    "    y_train_old = train_y_p1_help[:]\n",
    "    test_y_p1_help=np.argmax(test_y_p1,axis = -1)\n",
    "    y_test_old = test_y_p1_help[:] # save for error calculation\n",
    "    \n",
    "    #change the names of train_seq_x_p1, test_seq_x_p1, train_y_p1, test_y_p1\n",
    "    x_train=train_seq_x_p1\n",
    "    y_train=train_y_p1\n",
    "    x_test=test_seq_x_p1\n",
    "    y_test=test_y_p1\n",
    "    \n",
    "    n_trains = x_train.shape[0]\n",
    "    n_tests = x_test.shape[0]\n",
    "    \n",
    "    #initislize needed elements\n",
    "    train_accuracy_records = []\n",
    "    test_accuracy_records = []\n",
    "\n",
    "    probs = np.zeros((n_tests, num_classes))\n",
    "    pred_2 = np.zeros((n_tests, n_learners), dtype=\"int64\")\n",
    "       \n",
    "    for i in range(n_learners):\n",
    "        \n",
    "        train_picks = np.random.choice(n_trains, n_trains)\n",
    "        \n",
    "        if random:\n",
    "            x_train_i = x_train[train_picks, :]\n",
    "            y_train_i = y_train[train_picks, :]\n",
    "        else:\n",
    "            x_train_i = x_train\n",
    "            y_train_i = y_train\n",
    "            \n",
    "        epochs = epochs_lst[i]\n",
    "        \n",
    "        kill_model()\n",
    "        model = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1)            \n",
    "        model, history = train(x_train_i, y_train_i, x_test, y_test, model, batch_size, epochs)\n",
    "\n",
    "        print(\"model %d finished\" % (i))\n",
    "        scores = model.evaluate(x_train, y_train, verbose=1)\n",
    "        train_accuracy_records.append(scores[1])\n",
    "\n",
    "        #save the evaluation score\n",
    "        scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "        test_accuracy_records.append(scores[1])\n",
    "        print(\"accuracy evaluate= \" + str(scores[1]))\n",
    "        \n",
    "        pred=model.predict(x_test)\n",
    "        probs = probs + pred\n",
    "        \n",
    "        ''' return final_predict based on majority vote of all the learners in models'''\n",
    "        \n",
    "        pred_2[:, i] = predict(model, x_test) # each column stores one learner's prediction\n",
    "\n",
    "    print(\"Final scores:\")\n",
    "    #final predict weighted_vote     \n",
    "    final_predict = np.argmax(probs, axis=1)\n",
    "    errors = np.count_nonzero(final_predict.reshape((n_tests, )) - y_test_old.reshape((n_tests,)))\n",
    "    \n",
    "    #final predict majority vote     \n",
    "    final_predict_2 = np.zeros((n_tests, 1), dtype=\"int64\")\n",
    "    for i in range(n_tests):\n",
    "        final_predict_2[i] = mostcommon(pred_2[i, :])\n",
    "    errors_2 = np.count_nonzero(final_predict_2.reshape((n_tests, )) - y_test_old.reshape((n_tests,))) \n",
    "\n",
    "    print(\"Random = \" + str(random))\n",
    "    print('ensemble test accuracy weighted_vote: %f' % ((n_tests - errors)/float(n_tests)))\n",
    "    print('ensemble test accuracy majority vote: %f' % ((n_tests - errors_2)/float(n_tests)))\n",
    "\n",
    "    for i in range(n_learners):\n",
    "        print(\"learner %d (epochs = %d): %0.6f\" % (i, epochs_lst[i], test_accuracy_records[i]))\n",
    "\n",
    "    #Store final predictions\n",
    "    df=pd.DataFrame(probs)\n",
    "    df.sort_values(by=0, axis=1, ascending=False)\n",
    "    df.to_csv('F:/PhD/Datasets/ensemble/adaboost/predictions_bag_title_3.csv', header=False, index=False)\n",
    "\n",
    "    # Store rel\n",
    "    q_rel=y_test_old.reshape((n_tests,))\n",
    "    df_q_rel=pd.DataFrame(q_rel)\n",
    "    df_q_rel.to_csv('F:/PhD/Datasets/ensemble/adaboost/qrel_numbers_bag_tittle_3.csv', header=False, index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281cc6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of an encoded label/target \n",
      " IPC code:  A61K \n",
      " One-hot encoding: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "Number of ipc codes:  731 \n",
      "\n",
      "split_abstract_dataset-Done! \n",
      "\n",
      "Number of train data: 432904\n",
      "Number of validation data: 54114\n",
      "Number of test data: 54113 \n",
      "\n",
      "Number of unique words: 174089 \n",
      "\n",
      "convert text to tokens - Done! \n",
      "\n",
      "load_patentVec-Done! \n",
      "\n",
      "Bi-directional LSTM\n"
     ]
    }
   ],
   "source": [
    "words = [60]\n",
    "epochs_lst = [15, 15, 15]\n",
    "batch_size = 128\n",
    "n_learners = 3\n",
    "random = True\n",
    "\n",
    "for i, number_of_words in enumerate(words):\n",
    "    \n",
    "    trainDF_p1 = load_patents(\"F:/PhD/Datasets/ensemble/metadata_exploration/l3_dataset/abstract.csv\")\n",
    "\n",
    "    onehot_encoded, onehot_encoder=encode_labels(trainDF_p1)    \n",
    "   \n",
    "    number_of_codes=enumarate_codes(onehot_encoded)\n",
    "    \n",
    "    train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=split_dataset(trainDF_p1, onehot_encoded)\n",
    "\n",
    "    token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "    train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "    embeddings_index = load_fasttext('F:/PhD/Datasets/embeddings/patent-300.vec')\n",
    "\n",
    "    embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "    \n",
    "    bagging_train_model(n_learners, epochs_lst, batch_size, random, number_of_codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
