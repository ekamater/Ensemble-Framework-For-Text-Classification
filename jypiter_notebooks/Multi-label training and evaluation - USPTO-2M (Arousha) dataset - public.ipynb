{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84dc75d2",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dict=set(stopwords.words(\"english-v2-uspto-sklearn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc13e9",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f597e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv (with the 544 codes) from Arousha\n",
    "\n",
    "def load_train_data(number_of_words, part, file):\n",
    "\n",
    "### This function loads and prepares the train data\n",
    "    \n",
    "    trainDF = pd.read_csv(file, header=0)\n",
    "    \n",
    "    if part==0: #Title\n",
    "        del trainDF['text'], trainDF['Abstract']\n",
    "        trainDF=trainDF.rename(columns={'Title': 'text'})\n",
    "    if part==1: #Abstract\n",
    "        del trainDF['text'], trainDF['Title']    \n",
    "        trainDF=trainDF.rename(columns={'Abstract': 'text'})\n",
    "    if part==2: #Concat abstract and title\n",
    "        del testDF['Abstract'], testDF['Title']\n",
    "\n",
    "    #Process the text column\n",
    "    \n",
    "    trainDF['text']=trainDF['text'].fillna(\"\")\n",
    "    trainDF['text']=trainDF['text'].replace('[^a-z]', ' ', regex=True)       \n",
    "    trainDF['text']=trainDF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_dict)]))\n",
    "    trainDF['text']=trainDF['text'].str.split().str[0:number_of_words]\n",
    "    trainDF['text']=trainDF['text'].str.join(' ')\n",
    "\n",
    "    #Process the labels column\n",
    "    \n",
    "    trainDF['labels']=trainDF['labels'].str.split(',')\n",
    "    \n",
    "    print(\"Loaded\")\n",
    "   \n",
    "    return trainDF\n",
    "\n",
    "def load_test_data(number_of_words, part, file):\n",
    "\n",
    "### This function loads and prepares the test data\n",
    "\n",
    "    testDF = pd.read_csv(file, header=0)\n",
    "\n",
    "    if part==0: #Title\n",
    "        del testDF['text'], testDF['Abstract']\n",
    "        testDF=testDF.rename(columns={'Title': 'text'})\n",
    "    if part==1: #Abstract\n",
    "        del testDF['text'], testDF['Title']\n",
    "        testDF=testDF.rename(columns={'Abstract': 'text'})\n",
    "    if part==2: #Concat abstract and title\n",
    "        del testDF['Abstract'], testDF['Title']\n",
    "        \n",
    "    #Process the text column\n",
    "    \n",
    "    testDF['text']=testDF['text'].fillna(\"\")\n",
    "    testDF['text']=testDF['text'].replace('[^a-z]', ' ', regex=True)       \n",
    "    testDF['text']=testDF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_dict)]))\n",
    "    testDF['text']=testDF['text'].str.split().str[0:number_of_words]\n",
    "    testDF['text']=testDF['text'].str.join(' ')\n",
    "\n",
    "    #Process the labels column\n",
    "    \n",
    "    testDF['labels']=testDF['labels'].str.split(',')\n",
    "    \n",
    "    print(\"Loaded\")\n",
    "   \n",
    "    return testDF\n",
    "\n",
    "\n",
    "def encode_multilabel(trainDF):\n",
    "\n",
    "#This function encodes the labels with MultiLabelBinarizer\n",
    "        \n",
    "    labels_val=trainDF['labels']\n",
    "    \n",
    "    multihop_encoder = MultiLabelBinarizer()\n",
    "    multihop_encoded = multihop_encoder.fit_transform(labels_val)\n",
    "    \n",
    "    return multihop_encoder, multihop_encoded\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of codes: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "        \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "    \n",
    "    maxlen=number_of_words\n",
    "\n",
    "    seq_x = sequence.pad_sequences(token.texts_to_sequences(x), maxlen)\n",
    "    print('convert text to tokens - Done! \\n')\n",
    "\n",
    "    return seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    print(\"load_patentVec-Done! \\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "\n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "        \n",
    "def calculate_probabilities(multihop_encoded_train):\n",
    "\n",
    "# This function transforms the multilabel encoding into probabilities, e.g. 1 1 0 0 -> 0.5 0.5 0 0  \n",
    "\n",
    "\n",
    "    a = np.zeros((multihop_encoded_train.shape))\n",
    "\n",
    "    for i in range(len(multihop_encoded_train)):\n",
    "        sum_of_secondary_codes=sum(multihop_encoded_train[i])\n",
    "        #print(sum_of_secondary_codes)\n",
    "\n",
    "        for j in range(len(multihop_encoded_train[i])):\n",
    "            if multihop_encoded_train[i][j]==1:\n",
    "                a[i][j]=float(1/sum_of_secondary_codes)\n",
    "    print('calculate_probabilities-Done! \\n')\n",
    "\n",
    "    return a\n",
    "\n",
    "def create_bidirectional_lstm_probabilities_kullback(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "\n",
    "#This function creates a classification model\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    \n",
    "    # Add the output Layers\n",
    "   # output_layer1 = layers.Dropout(0.25)(lstm_layer)\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "\n",
    "#this function makes predictions on test data based on trained model\n",
    "\n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('make_predictions-Done! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true\n",
    "\n",
    "def calculate_metrix(number_of_test_samples, number_of_codes, predictions_kull, nn, multihop_encoded_test):\n",
    "\n",
    "# This function calculates the precision, recall and f1 score metrics\n",
    "\n",
    "    nn=nn+1\n",
    "    pred_class_kull=np.empty((number_of_test_samples, number_of_codes))\n",
    "    for row in range(number_of_test_samples):\n",
    "        predictions_p1_sort2=np.argsort(predictions_kull[row])[:-nn:-1]\n",
    "        class_number_zeros=np.zeros(number_of_codes)\n",
    "        for class_number in predictions_p1_sort2:\n",
    "            class_number_zeros[class_number]=1\n",
    "\n",
    "        pred_class_kull[row][:]=class_number_zeros\n",
    "    #print(pred_class_kull.shape)                      \n",
    "    print(metrics.precision_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.recall_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.f1_score(multihop_encoded_test, pred_class_kull, average='micro')*100)                    \n",
    "    \n",
    "    return None\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486e8e2",
   "metadata": {},
   "source": [
    "### Load the dataset and run the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08b562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "Loaded\n",
      "Number of codes:  544 \n",
      "\n",
      "Number of unique words: 93923 \n",
      "\n",
      "convert text to tokens - Done! \n",
      "\n",
      "convert text to tokens - Done! \n",
      "\n",
      "load_patentVec-Done! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "calculate_probabilities-Done! \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          28177200  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 544)               109344    \n",
      "=================================================================\n",
      "Total params: 28,607,344\n",
      "Trainable params: 430,144\n",
      "Non-trainable params: 28,177,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1947223/1947223 [==============================] - 3209s 2ms/step - loss: 1.9950 - acc: 0.4722\n",
      "make_predictions-Done! \n",
      "\n",
      "(49888, 544)\n",
      "micro! \n",
      "\n",
      "68.40723220012829\n",
      "35.59753413512188\n",
      "46.82725357958794\n",
      "macro! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.38131941871751\n",
      "19.11951662485183\n",
      "24.88365475187849\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 100\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "parts = [0, 1, 2] #0: Title, 1: Abstract, 2: Concatenated title and abstract \n",
    "\n",
    "for j, part in enumerate(parts):\n",
    "        \n",
    "    trainDF=load_train_data(number_of_words, part,'/datasets/USPTO_train_data_544.csv')     \n",
    "    testDF=load_test_data(number_of_words, part,'/datasets/USPTO_test_data_544.csv')\n",
    "\n",
    "    multihop_encoder, encoded_trainDF=encode_multilabel(trainDF)\n",
    "   \n",
    "    ### Encode the labels of test data using the multihop_encoder created on train data\n",
    "    labels_val_2=testDF['labels']\n",
    "    encoded_testDF = multihop_encoder.fit_transform(labels_val_2)\n",
    "    \n",
    "    number_of_codes=enumarate_codes(encoded_testDF)\n",
    "\n",
    "    token_p1, word_index_p1=tokenize_text(trainDF.append(testDF))\n",
    "\n",
    "    train_seq_x_p1 =convert_text(number_of_words, token_p1, trainDF['text'])   \n",
    "    test_seq_x_p1 =convert_text(number_of_words, token_p1, testDF['text'])\n",
    "\n",
    "    if part == 0:\n",
    "        embeddings_index = load_language_model('/embeddings/patent-300.vec')\n",
    "    embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "    #del embeddings_index\n",
    "    \n",
    "    kill_model()\n",
    "\n",
    "    a=calculate_probabilities(encoded_trainDF)\n",
    "\n",
    "    classifier = create_bidirectional_lstm_probabilities_kullback(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    history=classifier.fit(train_seq_x_p1, a, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    predictions_kull, prediction_kull, y_true_kull=make_predictions(test_seq_x_p1, encoded_testDF, classifier)\n",
    "\n",
    "    number_of_test_data_p1=np.shape(test_seq_x_p1)\n",
    "    number_of_test_data_p1=number_of_test_data_p1[0]\n",
    "\n",
    "    #calculate the P@1, R@1 and F1@1 vs all labels\n",
    "    calculate_metrix(number_of_test_data_p1, number_of_codes, predictions_kull, 1, encoded_testDF)\n",
    "    #calculate the P@3, R@3 and F1@3 vs all labels\n",
    "    calculate_metrix(number_of_test_data_p1, number_of_codes, predictions_kull, 3, encoded_testDF)\n",
    "    #calculate the P@5, R@5 and F1@5 vs all labels\n",
    "    calculate_metrix(number_of_test_data_p1, number_of_codes, predictions_kull, 5, encoded_testDF)\n",
    "    \n",
    "    #Save the final predictions\n",
    "    #df=pd.DataFrame(predictions_kull)\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #df.to_csv(\"uspto_arousha_part\"+str(part)+\".csv\", header=False, index=False)\n",
    "    #Save qrels\n",
    "    #df=pd.DataFrame(encoded_testDF)\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #df.to_csv('uspto_arousha_reference.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d746f6",
   "metadata": {},
   "source": [
    "### Load the stored predictions and qrel and create the ensemble of above classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5922141",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1=pd.read_csv('uspto_arousha_part0.csv',header=None)\n",
    "in2=pd.read_csv('uspto_arousha_part1.csv',header=None)\n",
    "in3=pd.read_csv('uspto_arousha_part2.csv',header=None)\n",
    "\n",
    "in1_predictions=in1.to_numpy()\n",
    "in2_predictions=in2.to_numpy()\n",
    "in3_predictions=in3.to_numpy()\n",
    "\n",
    "in1_prediction = np.argmax(in1_predictions, axis = -1) \n",
    "in2_prediction = np.argmax(in2_predictions, axis = -1) \n",
    "in3_prediction = np.argmax(in3_predictions, axis = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1203e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv('uspto_arousha_reference.csv',header=None)\n",
    "true=true.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_data_p1=in1_predictions.shape[0] \n",
    "number_of_codes=in1_predictions.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1271e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions3(predictions_p1, predictions_p2, predictions_p3, number_of_test_data_p1, en):\n",
    "\n",
    "#This functions calculates the averaging of predictions for each label\n",
    "\n",
    "    average_predictions = []\n",
    "    i=0\n",
    "    \n",
    "    for i in range(number_of_test_data_p1):\n",
    "        a=np.mean([predictions_p1[i], predictions_p2[i], predictions_p3[i]], axis=0)\n",
    "        average_predictions.append(a)\n",
    "    \n",
    "    average_predictions_2 = np.array(average_predictions)      \n",
    "    \n",
    "    average_prediction = np.argmax(average_predictions, axis = -1) \n",
    "\n",
    "    print('The ensemble predictions have been calculated! \\n')\n",
    "\n",
    "    return average_predictions_2, average_prediction\n",
    "\n",
    "ensembles=3 #the number of base classifiers combined using an avaraging function\n",
    "average_predictions, average_prediction=ensemble_predictions3(in1_predictions, in2_predictions, in3_predictions, number_of_test_data_p1, ensembles)\n",
    "\n",
    "#calculate the P@1, R@1 and F1@1 vs all labels\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 1, true)\n",
    "#calculate the P@3, R@3 and F1@3 vs all labels\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 3, true)\n",
    "#calculate the P@5, R@5 and F1@5 vs all labels\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 5, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
