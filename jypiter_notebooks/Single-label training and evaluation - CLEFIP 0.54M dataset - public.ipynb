{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d11b50",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents_single_label(file1):\n",
    "\n",
    "#This function loads the data\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[0,1])\n",
    "    trainDF=trainDF.rename(columns={0: 'label'})\n",
    "    trainDF=trainDF.rename(columns={1: 'text'})\n",
    "\n",
    "    return trainDF\n",
    "\n",
    "def load_patents_multi_label(file1):\n",
    "\n",
    "#This function loads the data\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[1,2])\n",
    "    trainDF=trainDF.rename(columns={2: 'labels'})\n",
    "    trainDF=trainDF.rename(columns={1: 'text'})\n",
    "\n",
    "    return trainDF\n",
    "\n",
    "def encode_single_labels(trainDF):\n",
    "    \n",
    "#This function encodes the labels with OneHotEncoder\n",
    "\n",
    "    labels_val=trainDF['label'].values\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(labels_val.reshape(-1, 1))\n",
    "    \n",
    "    return onehot_encoded, onehot_encoder\n",
    "\n",
    "def encode_multi_labels(trainDF, trainDF_targets):\n",
    "        \n",
    "    labels_val_main_target=trainDF['label']\n",
    "    labels_val_main_target = labels_val_main_target.str.split(',')\n",
    "    \n",
    "    labels_val=trainDF_targets['labels']\n",
    "    labels_val = labels_val.str.split(',')\n",
    "\n",
    "    labels_all=pd.concat([labels_val,labels_val_main_target])\n",
    "\n",
    "    multihop_encoder = MultiLabelBinarizer()\n",
    "    multihop_encoded = multihop_encoder.fit_transform(labels_all)\n",
    "    \n",
    "    splitted=np.array_split(multihop_encoded, 2)\n",
    "\n",
    "    return multihop_encoder, splitted[0], splitted[1]\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of codes: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def split_dataset(trainDF, onehot_encoded):\n",
    "\n",
    "# This function splits the data into train, validation and test set (80:10:10)\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], onehot_encoded, test_size=0.2, random_state=42) #stratify=onehot_encoded\n",
    "    test_x, valid_x, test_y, valid_y = train_test_split(valid_x, valid_y, test_size=0.5, random_state=41)\n",
    "        \n",
    "    #Number of data per split\n",
    "    \n",
    "    number_of_train_data=np.shape(train_x)\n",
    "    number_of_train_data=number_of_train_data[0]\n",
    "    print(\"Number of train data:\", number_of_train_data)\n",
    "\n",
    "    number_of_valid_data=np.shape(valid_x)\n",
    "    number_of_valid_data=number_of_valid_data[0]\n",
    "    print(\"Number of validation data:\",number_of_valid_data)\n",
    "\n",
    "    number_of_test_data=np.shape(test_x)\n",
    "    number_of_test_data=number_of_test_data[0]\n",
    "    print(\"Number of test data:\",number_of_test_data, \"\\n\")\n",
    "    \n",
    "    return train_x, train_y,  valid_x, valid_y, test_x, test_y, number_of_test_data\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "        \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, train_x, valid_x, test_x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "    \n",
    "    maxlen=number_of_words\n",
    "\n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen)\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen)\n",
    "    test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen)\n",
    "    print('convert text to tokens - Done! \\n')\n",
    "\n",
    "    return train_seq_x, valid_seq_x, test_seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    \n",
    "    print(\"load_patentVec-Done! \\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "    \n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def calculate_probabilities(multihop_encoded_train):\n",
    "\n",
    "# This function transforms the multilabel encoding into probabilities, e.g. 1 1 0 0 -> 0.5 0.5 0 0  \n",
    "\n",
    "\n",
    "    a = np.zeros((multihop_encoded_train.shape))\n",
    "\n",
    "    for i in range(len(multihop_encoded_train)):\n",
    "        sum_of_secondary_codes=sum(multihop_encoded_train[i])\n",
    "        #print(sum_of_secondary_codes)\n",
    "\n",
    "        for j in range(len(multihop_encoded_train[i])):\n",
    "            if multihop_encoded_train[i][j]==1:\n",
    "                a[i][j]=float(1/sum_of_secondary_codes)\n",
    "    print('calculate_probabilities-Done! \\n')\n",
    "\n",
    "    return a\n",
    "\n",
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "\n",
    "#this function makes predictions on test data based on trained model\n",
    "\n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('make_predictions-Done! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true\n",
    "\n",
    "def calculate_metrics_single_label(predictions, prediction, y_true, number_of_test_data):\n",
    "    \n",
    "    #Accuracy \n",
    "    accuracy_total=metrics.accuracy_score(prediction, y_true)*100\n",
    "    print(\"Accuracy:\", accuracy_total)\n",
    "\n",
    "    #MRR, P@3, P@5, P@10\n",
    "    all_rr=[]\n",
    "    number_of_top_three=0\n",
    "    number_of_top_five=0\n",
    "    number_of_top_ten=0\n",
    "\n",
    "    predictions_2=predictions.argsort()\n",
    "    predictions_3=np.fliplr(predictions_2)\n",
    "    for i in range (0, number_of_test_data):\n",
    "        specific_prediction=predictions_3[i,:]\n",
    "        list1 = specific_prediction.tolist()\n",
    "        target=y_true[i]\n",
    "        prediction_rank=list1.index(target)+1 \n",
    "        #MRR\n",
    "        RR=1/prediction_rank\n",
    "        all_rr.append(RR)\n",
    "        #P@3\n",
    "        if prediction_rank<= 3:\n",
    "            number_of_top_three=number_of_top_three+1\n",
    "        #P@5\n",
    "        if prediction_rank<= 5:\n",
    "            number_of_top_five=number_of_top_five+1     \n",
    "        #P@10\n",
    "        if prediction_rank<= 10:\n",
    "            number_of_top_ten=number_of_top_ten+1 \n",
    "    MRR=np.mean(all_rr)\n",
    "    print(\"MRR:\", MRR)\n",
    "    P3=number_of_top_three/number_of_test_data*100\n",
    "    print(\"P@3:\", P3)\n",
    "    P5=number_of_top_five/number_of_test_data*100\n",
    "    print(\"P@5:\", P5)\n",
    "    P10=number_of_top_ten/number_of_test_data*100\n",
    "    print(\"P@10:\", P10)\n",
    "    \n",
    "    return accuracy_total, MRR, P3, P5, P10\n",
    "\n",
    "def calculate_metrics_multi_label(number_of_test_samples, number_of_codes, predictions_kull, nn, multihop_encoded_test):\n",
    "\n",
    "# This function calculates the precision, recall and f1 score metrics\n",
    "\n",
    "    nn=nn+1\n",
    "    pred_class_kull=np.empty((number_of_test_samples, number_of_codes))\n",
    "    for row in range(number_of_test_samples):\n",
    "        predictions_p1_sort2=np.argsort(predictions_kull[row])[:-nn:-1]\n",
    "        class_number_zeros=np.zeros(number_of_codes)\n",
    "        for class_number in predictions_p1_sort2:\n",
    "            class_number_zeros[class_number]=1\n",
    "\n",
    "        pred_class_kull[row][:]=class_number_zeros\n",
    "    #print(pred_class_kull.shape)                      \n",
    "    print(metrics.precision_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.recall_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.f1_score(multihop_encoded_test, pred_class_kull, average='micro')*100)                    \n",
    "    \n",
    "    return None\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0ccdf",
   "metadata": {},
   "source": [
    "### Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc5ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    \n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_200(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    \n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(200, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_bidirectional_gru(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "\n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "    \n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "\n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1)(embedding_layer)\n",
    "    \n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gru(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "\n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "  \n",
    "    # Add the output layers\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_probabilities_kullback(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "\n",
    "#This function creates a classification model\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "\n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "    \n",
    "    # Add the output layers\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "### Main 1: Different sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unnecessary-australian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of codes:  731 \n",
      "\n",
      "Number of train data: 432904\n",
      "Number of validation data: 54114\n",
      "Number of test data: 54113 \n",
      "\n",
      "Number of unique words: 66352 \n",
      "\n",
      "convert text to tokens - Done! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 300)           19905900  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 731)               146931    \n",
      "=================================================================\n",
      "Total params: 20,373,631\n",
      "Trainable params: 467,731\n",
      "Non-trainable params: 19,905,900\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "432904/432904 [==============================] - 427s 985us/step - loss: 2.6136 - acc: 0.4168\n",
      "make_predictions-Done! \n",
      "\n",
      "Accuracy: 52.37743240995694\n",
      "MRR: 0.6493095650918518\n",
      "P@3: 73.57012178219652\n",
      "P@5: 80.77356642581265\n",
      "P@10: 87.98440300851921\n"
     ]
    }
   ],
   "source": [
    "number_of_words=60\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "parts=[1, 2, 3, 4]\n",
    "\n",
    "file1= \"/datasets/title.csv\"\n",
    "file2= \"/datasets/abstract.csv\"\n",
    "file3 = \"/datasets/description.csv\"\n",
    "file4 = \"/datasets/claims.csv\"\n",
    "\n",
    "for i, part in enumerate(parts):\n",
    "    \n",
    "    print(\"Part: \", part)\n",
    "    \n",
    "    if part==1:\n",
    "        trainDF_p1 = load_patents_single_label(file1)\n",
    "    if part==2:\n",
    "        trainDF_p1 = load_patents_single_label(file2)\n",
    "    if part==3:\n",
    "        trainDF_p1 = load_patents_single_label(file3)\n",
    "    if part==4:\n",
    "        trainDF_p1 = load_patents_single_label(file4)\n",
    "   \n",
    "    onehot_encoded, onehot_encoder=encode_single_labels(trainDF_p1)        \n",
    "    number_of_codes=enumarate_codes(onehot_encoded)\n",
    "    \n",
    "    train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=split_dataset\\\n",
    "    (trainDF_p1, onehot_encoded)\n",
    "    \n",
    "    token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "    train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "    if part==1:\n",
    "        embeddings_index = load_language_model('/embeddings/patent-300.vec')\n",
    "    embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "    \n",
    "    kill_model()\n",
    "    classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    history1=classifier1.fit(train_seq_x_p1, train_y_p1, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_p1, classifier1)\n",
    "    accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(predictions_p1, prediction_p1, y_true1, number_of_test_data_p1)\n",
    "    \n",
    "    #Save the final predictions\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"clefip_part\"+str(part)\".csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)\n",
    "        \n",
    "    #Save qrel\n",
    "    #df=pd.DataFrame(y_true1)\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"clefip_qrel.csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad258ffb",
   "metadata": {},
   "source": [
    "### Main 2: Different classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words=60\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "cl_models=[1, 2, 3, 4]\n",
    "\n",
    "file1= \"/datasets/abstract.csv\"\n",
    "\n",
    "trainDF_p1 = load_patents_single_label(file1)\n",
    "\n",
    "onehot_encoded, onehot_encoder=encode_single_labels(trainDF_p1)        \n",
    "number_of_codes=enumarate_codes(onehot_encoded)\n",
    "\n",
    "train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=\\\n",
    "split_dataset(trainDF_p1, onehot_encoded)\n",
    "\n",
    "token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "embeddings_index = load_language_model('/embeddings/patent-300.vec')\n",
    "embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "\n",
    "#for releasing resources we can delete the language model after creating the embedding matrix\n",
    "#del embeddings_index\n",
    "\n",
    "kill_model()\n",
    "\n",
    "for i, cl_model in enumerate(cl_models):\n",
    "    \n",
    "    if cl_model==1:\n",
    "        classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    if cl_model==2:\n",
    "        classifier1 = create_bidirectional_gru(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    if cl_model==3:\n",
    "        classifier1 = create_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    if cl_model==4:\n",
    "        classifier1 = create_gru(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    \n",
    "    history1=classifier1.fit(train_seq_x_p1, train_y_p1, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_p1, classifier1)\n",
    "    accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(predictions_p1, prediction_p1, y_true1, number_of_test_data_p1)\n",
    "\n",
    "    #Save the final predictions\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"clefip_model\"+str(model)\".csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96161669",
   "metadata": {},
   "source": [
    "### Main 3: Different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words=60\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "networks=[1, 2]\n",
    "\n",
    "file1= \"/datasets/abstract.csv\"\n",
    "\n",
    "trainDF_p1 = load_patents_single_label(file1)\n",
    "\n",
    "onehot_encoded, onehot_encoder=encode_single_labels(trainDF_p1) \n",
    "number_of_codes=enumarate_codes(onehot_encoded)\n",
    "\n",
    "train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=\\\n",
    "split_dataset(trainDF_p1, onehot_encoded)\n",
    "\n",
    "token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "embeddings_index = load_language_model('/embeddings/patent-300.vec')\n",
    "embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "#del embeddings_index\n",
    "\n",
    "kill_model()\n",
    "\n",
    "for i, network in enumerate(networks):\n",
    "    \n",
    "    if network==1:\n",
    "        classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "    if network==2:\n",
    "        classifier1 = create_bidirectional_lstm_200(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "   \n",
    "    history1=classifier1.fit(train_seq_x_p1, train_y_p1, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_p1, classifier1)\n",
    "    accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(predictions_p1, prediction_p1, y_true1, number_of_test_data_p1)\n",
    "\n",
    "    #Save the final predictions\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"clefip_network\"+str(network)\".csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467622bf",
   "metadata": {},
   "source": [
    "### Main 4: Different training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words=60\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "training_models=[1, 2] \n",
    "#1: get the main and all labels and encode them using MultiBinalizer, then, use only the main labels for the training\n",
    "#2: get all labels and encode them using MultiBinalizer, then, use all labels for the training \n",
    "\n",
    "file1= \"F:/PhD/Datasets-Results/clefip/Datasets/I3_dataset_multilabel/abstract.csv\"\n",
    "\n",
    "trainDF_p1 = load_patents_single_label(file1)\n",
    "trainDF_p1_multi = load_patents_multi_label(file1)\n",
    "\n",
    "multihop_encoder, multihop_encoded_all, multihop_encoded_main=encode_multi_labels(trainDF_p1, trainDF_p1_multi)       \n",
    "number_of_codes=enumarate_codes(multihop_encoded_all)\n",
    "\n",
    "train_x_main, train_y_main, valid_x_main, valid_y_main, test_x_main, test_y_main, number_of_test_data_main=\\\n",
    "split_dataset(trainDF_p1, multihop_encoded_main)   \n",
    "train_x_all, train_y_all, valid_x_all, valid_y_all, test_x_all, test_y_all, number_of_test_data_all=\\\n",
    "split_dataset(trainDF_p1_multi, multihop_encoded_all)\n",
    "\n",
    "a=calculate_probabilities(multihop_encoded_all)\n",
    "\n",
    "train_x_a, train_y_a, valid_x_a, valid_y_a, test_x_a, test_y_a, number_of_test_data_a=split_dataset(trainDF_p1, a)\n",
    "\n",
    "token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_main, valid_x_main, test_x_main)\n",
    "\n",
    "embeddings_index = load_language_model('F:/PhD/Datasets-Results/embeddings/patent-300.vec')\n",
    "embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "\n",
    "del embeddings_index\n",
    "\n",
    "kill_model()\n",
    "\n",
    "for i, training_model in enumerate(training_models):\n",
    "    \n",
    "    if training_model==1:\n",
    "        classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        history1=classifier1.fit(train_seq_x_p1, train_y_main, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_main, classifier1)\n",
    "        accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(predictions_p1, prediction_p1, y_true1, number_of_test_data_main)\n",
    "    \n",
    "    if training_model==2:  \n",
    "        classifier1 = create_bidirectional_lstm_probabilities_kullback(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        history1=classifier1.fit(train_seq_x_p1, train_y_a, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_main, classifier1)\n",
    "        accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(predictions_p1, prediction_p1, y_true1, number_of_test_data_main)\n",
    "    \n",
    "    #Save the final predictions\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #file_name=\"clefip_training_model\"+str(training_model)\".csv\"\n",
    "    #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0b5bb",
   "metadata": {},
   "source": [
    "### Load the stored predictions and qrel and create the ensemble of above classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9caa5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the predictions created by different classifiers\n",
    "in1=pd.read_csv('predictions_p1.csv',header=None)\n",
    "in2=pd.read_csv('predictions_p2.csv',header=None)\n",
    "in3=pd.read_csv('predictions_p3.csv',header=None)\n",
    "\n",
    "in1_predictions=in1.to_numpy()\n",
    "in2_predictions=in2.to_numpy()\n",
    "in3_predictions=in3.to_numpy()\n",
    "\n",
    "in1_prediction = np.argmax(in1_predictions, axis = -1) \n",
    "in2_prediction = np.argmax(in2_predictions, axis = -1) \n",
    "in3_prediction = np.argmax(in3_predictions, axis = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db3a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv('qrel_numbers.csv',header=None)\n",
    "true=true.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c99cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_data=in1_predictions.shape[0] \n",
    "number_of_codes=in1_predictions.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions3(predictions_p1, predictions_p2, predictions_p3, number_of_test_data_p1, en):\n",
    "\n",
    "#This functions calculates the averaging of predictions for each label\n",
    "\n",
    "    average_predictions = []\n",
    "    i=0\n",
    "    \n",
    "    for i in range(number_of_test_data_p1):\n",
    "        a=np.mean([predictions_p1[i], predictions_p2[i], predictions_p3[i]], axis=0)\n",
    "        average_predictions.append(a)\n",
    "    \n",
    "    average_predictions_2 = np.array(average_predictions)      \n",
    "    \n",
    "    average_prediction = np.argmax(average_predictions, axis = -1) \n",
    "\n",
    "    print('The ensemble predictions have been calculated! \\n')\n",
    "\n",
    "    return average_predictions_2, average_prediction\n",
    "\n",
    "ensembles=3 #the number of base classifiers combined using an avaraging function\n",
    "average_predictions, average_prediction=ensemble_predictions3(in1_predictions, in2_predictions, in3_predictions, number_of_test_data, ensembles)\n",
    "accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics_single_label(average_predictions, average_prediction, true, number_of_test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
