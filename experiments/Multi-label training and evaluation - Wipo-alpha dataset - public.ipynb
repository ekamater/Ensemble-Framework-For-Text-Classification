{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dict=set(stopwords.words(\"english-v2-uspto-sklearn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944ebbf",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a46282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents(number_of_words, file1):\n",
    "\n",
    "#This function loads the text and the labels from a csv file \n",
    "# number_of_words: the first number of words that we will use\n",
    "# file1: the csv file with the dataset containing all textual fields and labels\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[2,3])\n",
    "    \n",
    "    #labels\n",
    "    trainDF=trainDF.rename(columns={3: 'labels'})\n",
    "    \n",
    "    #text    \n",
    "    trainDF=trainDF.rename(columns={2: 'text'})\n",
    "    \n",
    "    #replace the na rows with \"\" otherwise it returns an error\n",
    "    trainDF['text']=trainDF['text'].fillna(\"\")\n",
    "    #delete all symbols except for a-z\n",
    "    trainDF['text']=trainDF['text'].replace('[^a-z]', ' ', regex=True)       \n",
    "    #delete the stopwords\n",
    "    trainDF['text']=trainDF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_dict)]))\n",
    "    #keep the first #number of words\n",
    "    trainDF['text']=trainDF['text'].str.split().str[0:number_of_words]\n",
    "    trainDF['text']=trainDF['text'].str.join(' ')\n",
    "    print(\"The data has been loaded! \\n\")\n",
    "   \n",
    "    return trainDF\n",
    "\n",
    "def load_main_target(file1):\n",
    "\n",
    "#This function loads the main labels from a csv file (the main labels are needed for the evaluation)\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[1])\n",
    "    trainDF=trainDF.rename(columns={1: 'label'})\n",
    "    print(\"The main labels have been loaded! \\n\")\n",
    "    \n",
    "    return trainDF\n",
    "\n",
    "def encode_multilabels(trainDF, trainDF_main_target, trainDF_test, trainDF_main_target_test):\n",
    "\n",
    "#This function concats all the available labels from train and test sets (including main labels) \n",
    "# and encodes them with MultiLabelBinarizer. \n",
    "    \n",
    "    #train    \n",
    "    labels_val=trainDF['labels']\n",
    "    labels_val = labels_val.str.split(',')\n",
    "    \n",
    "    labels_val_main_target=trainDF_main_target['label']\n",
    "    labels_val_main_target = labels_val_main_target.str.split(',')\n",
    "    \n",
    "    labels_all=pd.concat([labels_val,labels_val_main_target])\n",
    "    \n",
    "    #test\n",
    "    labels_val_test=trainDF_test['labels']\n",
    "    labels_val_test = labels_val_test.str.split(',')\n",
    "    \n",
    "    labels_val_main_target_test=trainDF_main_target_test['label']\n",
    "    labels_val_main_target_test = labels_val_main_target_test.str.split(',')\n",
    "    \n",
    "    labels_all_test=pd.concat([labels_val_test,labels_val_main_target_test])\n",
    "    \n",
    "    labels_all2=pd.concat([labels_all, labels_all_test])\n",
    "    \n",
    "    multihop_encoder = MultiLabelBinarizer()\n",
    "    multihop_encoded_original = multihop_encoder.fit_transform(labels_all2)\n",
    "         \n",
    "    multihop_encoded=multihop_encoded_original[0:labels_all.shape[0], :]\n",
    "    splitted=np.array_split(multihop_encoded, 2)\n",
    "    \n",
    "    multihop_encoded_test=multihop_encoded_original[labels_all.shape[0]:labels_all.shape[0]+labels_all_test.shape[0], :]\n",
    "    splitted_test=np.array_split(multihop_encoded_test, 2)\n",
    "    print(\"The labels have been encoded! \\n\")\n",
    "    \n",
    "    return multihop_encoder, splitted[0], splitted[1], splitted_test[0], splitted_test[1]\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of labels: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "        \n",
    "#This function tokenizes the text\n",
    "\n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "\n",
    "    maxlen=number_of_words\n",
    "\n",
    "    seq_x = sequence.pad_sequences(token.texts_to_sequences(x), maxlen)\n",
    "    print(\"The text has been converted to tokens! \\n\")\n",
    "\n",
    "    return seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    print(\"The word embeddings have been loaded! \\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "\n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def create_bidirectional_lstm_probabilities_kullback(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "\n",
    "# This function creates the classification model based on Bi-LSTM and KL loss\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the Embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    # Add the SpatialDropout1D Layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "    # Add a Bidirectional Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "    # Add the Output Layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "\n",
    "# This function makes the predictions in the test data\n",
    "\n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('The predictions on test data have been calculated! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f0cfd",
   "metadata": {},
   "source": [
    "### Special functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89061558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(multihop_encoded_train):\n",
    "\n",
    "# This function transforms the multilabel encoding into probabilities, e.g. 1 1 0 0 -> 0.5 0.5 0 0  \n",
    "\n",
    "    a = np.zeros((multihop_encoded_train.shape))\n",
    "\n",
    "    for i in range(len(multihop_encoded_train)):\n",
    "        sum_of_secondary_codes=sum(multihop_encoded_train[i])-1\n",
    "\n",
    "        for j in range(len(multihop_encoded_train[i])):\n",
    "            if multihop_encoded_train[i][j]==1:\n",
    "                if sum_of_secondary_codes>0:\n",
    "                    a[i][j]=float(0.4/sum_of_secondary_codes)\n",
    "            if multihop_encoded_train_main_target[i][j]==1:\n",
    "                if sum_of_secondary_codes>0:\n",
    "                    a[i][j]=float(0.6)\n",
    "                else:\n",
    "                    a[i][j]=1\n",
    "\n",
    "    print('The probabilities have been calculated! \\n')\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31c35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrix(number_of_test_samples, number_of_codes, predictions_kull, nn, multihop_encoded_test):\n",
    "\n",
    "# This function calculates the precision, recall and f1 score metrics\n",
    "\n",
    "    nn=nn+1\n",
    "    pred_class_kull=np.empty((number_of_test_samples, number_of_codes))\n",
    "    for row in range(number_of_test_samples):\n",
    "        predictions_p1_sort2=np.argsort(predictions_kull[row])[:-nn:-1]\n",
    "        class_number_zeros=np.zeros(number_of_codes)\n",
    "        for class_number in predictions_p1_sort2:\n",
    "            class_number_zeros[class_number]=1\n",
    "\n",
    "        pred_class_kull[row][:]=class_number_zeros\n",
    "    print(pred_class_kull.shape)                      \n",
    "    print(metrics.precision_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.recall_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.f1_score(multihop_encoded_test, pred_class_kull, average='micro')*100)                    \n",
    "    print('calculate_metrix-Done! \\n')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17963ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been loaded! \n",
      "\n",
      "The main labels have been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The main labels have been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  633 \n",
      "\n",
      "Number of unique words: 25606 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 300)           7682100   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 633)               127233    \n",
      "=================================================================\n",
      "Total params: 8,130,133\n",
      "Trainable params: 448,033\n",
      "Non-trainable params: 7,682,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "46322/46322 [==============================] - 47s 1ms/step - loss: 4.1866 - acc: 0.1702\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "calculate the P@1, R@1 and F1@1 vs all labels\n",
      "(28925, 633)\n",
      "34.81417458945549\n",
      "23.13134561492167\n",
      "27.79502891290247\n",
      "calculate_metrix-Done! \n",
      "\n",
      "calculate the P@1, R@1 and F1@1 vs main label\n",
      "(28925, 633)\n",
      "27.50561797752809\n",
      "27.50561797752809\n",
      "27.50561797752809\n",
      "calculate_metrix-Done! \n",
      "\n",
      "calculate the P@3, R@3 and F1@3 vs main label\n",
      "(28925, 633)\n",
      "16.334197637568423\n",
      "49.002592912705275\n",
      "24.501296456352637\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The main labels have been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The main labels have been loaded! \n",
      "\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  633 \n",
      "\n",
      "Number of unique words: 61418 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 60, 300)           18425700  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 633)               127233    \n",
      "=================================================================\n",
      "Total params: 18,873,733\n",
      "Trainable params: 448,033\n",
      "Non-trainable params: 18,425,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "46322/46322 [==============================] - 48s 1ms/step - loss: 4.5049 - acc: 0.1032\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "calculate the P@1, R@1 and F1@1 vs all labels\n",
      "(28925, 633)\n",
      "24.87121866897148\n",
      "16.52501493085864\n",
      "19.856746573924564\n",
      "calculate_metrix-Done! \n",
      "\n",
      "calculate the P@1, R@1 and F1@1 vs main label\n",
      "(28925, 633)\n",
      "18.990492653414\n",
      "18.990492653414\n",
      "18.990492653413998\n",
      "calculate_metrix-Done! \n",
      "\n",
      "calculate the P@3, R@3 and F1@3 vs main label\n",
      "(28925, 633)\n",
      "12.427542494958226\n",
      "37.28262748487467\n",
      "18.641313742437337\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-791a88c1394d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtrain_DF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_patents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtrain_DF_main_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_main_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d66937cd2755>\u001b[0m in \u001b[0;36mload_main_target\u001b[1;34m(file1)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#This function loads the main labels from a csv file (the main labels are needed for the evaluation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtrainDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtrainDF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The main labels have been loaded! \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow_3_6_13_basic\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2157\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2158\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "words = [60]\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "parts=[1, 2, 3, 4] #1: title, 2: abstract, 3: description, 4: claims\n",
    "\n",
    "file1= \"/datasets/wipo_train_patents_had_level3_main_title_multilabel.csv\"\n",
    "file2= \"/datasets/wipo_test_patents_had_level3_main_title_multilabel.csv\"\n",
    "file3= \"/datasets/wipo_train_patents_had_level3_main_abstract_multilabel.csv\"\n",
    "file4= \"/datasets/wipo_test_patents_had_level3_main_abstract_multilabel.csv\"\n",
    "file5= \"/datasets/wipo_train_patents_had_level3_main_description_multilabel.csv\"\n",
    "file6= \"/datasets/wipo_test_patents_had_level3_main_description_multilabel.csv\"\n",
    "file7= \"/datasets/wipo_train_patents_had_level3_main_claims_multilabel.csv\"\n",
    "file8= \"/datasets/wipo_test_patents_had_level3_main_claims_multilabel.csv\"\n",
    "\n",
    "for i, number_of_words in enumerate(words):\n",
    "    for k, part in enumerate(parts):\n",
    "    \n",
    "        if part == 1:\n",
    "            f1=file1\n",
    "            f2=file2\n",
    "        if part == 2:\n",
    "            f1=file3\n",
    "            f2=file4        \n",
    "        if part == 3:\n",
    "            f1=file5\n",
    "            f2=file6        \n",
    "        if part == 4:\n",
    "            f1=file7\n",
    "            f2=file8\n",
    "            \n",
    "        #train data   \n",
    "        train_DF=load_patents(number_of_words,f1)\n",
    "        train_DF_main_target = load_main_target(f1)\n",
    "\n",
    "        #test data\n",
    "        test_DF=load_patents(number_of_words,f2)\n",
    "        test_DF_main_target = load_main_target(f2)\n",
    "\n",
    "        multihop_encoder, multihop_encoded_train, multihop_encoded_train_main_target, \\\n",
    "        multihop_encoded_test, multihop_encoded_test_main_target=encode_multilabels\\\n",
    "        (train_DF, train_DF_main_target, test_DF, test_DF_main_target)\n",
    "\n",
    "        number_of_codes=enumarate_codes(multihop_encoded_train)  \n",
    "\n",
    "        combined=pd.concat([train_DF, test_DF])\n",
    "        token_p1, word_index_p1=tokenize_text(combined)\n",
    "\n",
    "        train_seq_x_p1 =convert_text(number_of_words, token_p1, train_DF['text'])\n",
    "        test_seq_x_p1 =convert_text(number_of_words, token_p1, test_DF['text'])\n",
    "\n",
    "        if part==1: \n",
    "            embeddings_index = load_language_model('/embeddings/patent-300.vec')\n",
    "        embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "\n",
    "        a=calculate_probabilities(multihop_encoded_train)\n",
    "        \n",
    "        classifier = create_bidirectional_lstm_probabilities_kullback(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        history3=classifier.fit(train_seq_x_p1, a, epochs=epochs, batch_size=128, verbose=1)\n",
    "\n",
    "        predictions_kull, prediction_kull, y_true_kull=make_predictions(test_seq_x_p1, multihop_encoded_test, classifier)\n",
    "        \n",
    "        number_of_test_data=np.shape(test_seq_x_p1)\n",
    "        number_of_test_data=number_of_test_data[0]\n",
    "        \n",
    "        print(\"calculate the P@1, R@1 and F1@1 vs all labels\")\n",
    "        calculate_metrix(number_of_test_data, number_of_codes, predictions_kull, 1, multihop_encoded_test)\n",
    "        print(\"calculate the P@1, R@1 and F1@1 vs main label\")\n",
    "        calculate_metrix(number_of_test_data, number_of_codes, predictions_kull, 1, multihop_encoded_test_main_target)\n",
    "        print(\"calculate the P@3, R@3 and F1@3 vs main label\")\n",
    "        calculate_metrix(number_of_test_data, number_of_codes, predictions_kull, 3, multihop_encoded_test_main_target)\n",
    "        \n",
    "        #Save the final predictions\n",
    "        #df=pd.DataFrame(predictions_kull)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #df.to_csv(\"wipo_\"+str(part)+\"_\"+str(words)+\"_30Epoch.csv\", header=False, index=False)   \n",
    "        \n",
    "        #Save qrels multilabel\n",
    "        #df=pd.DataFrame(multihop_encoded_test)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #df.to_csv('multihop_encoded_test.csv', header=False, index=False)\n",
    "        \n",
    "        #Save qrels main label\n",
    "        #df=pd.DataFrame(multihop_encoded_test_main_target)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #df.to_csv('multihop_encoded_test_main_target.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634aac3c",
   "metadata": {},
   "source": [
    "### Load the stored predictions and qrel and create the ensemble of above classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1=pd.read_csv('wipo_1_60_30Epoch.csv',header=None)\n",
    "in2=pd.read_csv('wipo_2_60_30Epoch.csv',header=None)\n",
    "in3=pd.read_csv('wipo_3_60_30Epoch.csv',header=None)\n",
    "in4=pd.read_csv('wipo_4_60_30Epoch.csv',header=None)\n",
    "\n",
    "in1_predictions=in1.to_numpy()\n",
    "in2_predictions=in2.to_numpy()\n",
    "in3_predictions=in3.to_numpy()\n",
    "in4_predictions=in4.to_numpy()\n",
    "\n",
    "in1_prediction = np.argmax(in1_predictions, axis = -1) \n",
    "in2_prediction = np.argmax(in2_predictions, axis = -1) \n",
    "in3_prediction = np.argmax(in3_predictions, axis = -1) \n",
    "in4_prediction = np.argmax(in4_predictions, axis = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdb212",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv('multihop_encoded_test.csv',header=None)\n",
    "true=true.to_numpy()\n",
    "\n",
    "true_main=pd.read_csv('multihop_encoded_test_main_target.csv',header=None)\n",
    "true_main=true_main.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826db37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_data_p1=in1_predictions.shape[0] \n",
    "number_of_codes=in1_predictions.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions4(predictions_p1, predictions_p2,predictions_p3, predictions_p4, number_of_test_data_p1, en):\n",
    "\n",
    "#This functions calculates the averaging of predictions for each label\n",
    "    \n",
    "    average_predictions = []\n",
    "    i=0\n",
    "    \n",
    "    for i in range(number_of_test_data_p1):\n",
    "        a=np.mean([predictions_p1[i], predictions_p2[i], predictions_p3[i], predictions_p4[i]], axis=0)\n",
    "        average_predictions.append(a)\n",
    "    \n",
    "    average_predictions_2 = np.array(average_predictions)      \n",
    "    \n",
    "    average_prediction = np.argmax(average_predictions, axis = -1) \n",
    "\n",
    "    print('The ensemble predictions have been calculated! \\n')\n",
    "\n",
    "    return average_predictions_2, average_prediction\n",
    "\n",
    "ensembles=4 #the number of base classifiers combined using an avaraging function\n",
    "average_predictions, average_prediction=ensemble_predictions4(in1_predictions, in2_predictions, in3_predictions, in4_predictions, number_of_test_data_p1, ensembles)\n",
    "\n",
    "#calculate the P@1, R@1 and F1@1 vs all labels\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 1, true)\n",
    "#calculate the P@1, R@1 and F1@1 vs main label\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 1, true_main)\n",
    "#calculate the P@3, R@3 and F1@3 vs main label\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 3, true_main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
