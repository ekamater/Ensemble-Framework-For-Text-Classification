{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d11b50",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents(file1):\n",
    "    \n",
    "# This function loads the data\n",
    "\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[0,1])\n",
    "    trainDF=trainDF.rename(columns={0: 'label'})\n",
    "    trainDF=trainDF.rename(columns={1: 'text'})\n",
    "\n",
    "    return trainDF\n",
    "\n",
    "def encode_labels(trainDF):\n",
    "    \n",
    "#This function encodes the labels with OneHotEncoder\n",
    "\n",
    "    labels_val=trainDF['label'].values\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(labels_val.reshape(-1, 1))\n",
    "    \n",
    "    return onehot_encoded, onehot_encoder\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of codes: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def split_dataset(trainDF, onehot_encoded):\n",
    "\n",
    "# This function splits the data into train, validation and test set (80:10:10)\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], onehot_encoded, test_size=0.2, random_state=42) #stratify=onehot_encoded\n",
    "    test_x, valid_x, test_y, valid_y = train_test_split(valid_x, valid_y, test_size=0.5, random_state=41)\n",
    "        \n",
    "    #Number of data per split\n",
    "    \n",
    "    number_of_train_data=np.shape(train_x)\n",
    "    number_of_train_data=number_of_train_data[0]\n",
    "    print(\"Number of train data:\", number_of_train_data)\n",
    "\n",
    "    number_of_valid_data=np.shape(valid_x)\n",
    "    number_of_valid_data=number_of_valid_data[0]\n",
    "    print(\"Number of validation data:\",number_of_valid_data)\n",
    "\n",
    "    number_of_test_data=np.shape(test_x)\n",
    "    number_of_test_data=number_of_test_data[0]\n",
    "    print(\"Number of test data:\",number_of_test_data, \"\\n\")\n",
    "    \n",
    "    return train_x, train_y,  valid_x, valid_y, test_x, test_y, number_of_test_data\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "        \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, train_x, valid_x, test_x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "    \n",
    "    maxlen=number_of_words\n",
    "\n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen)\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen)\n",
    "    test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen)\n",
    "    print('convert text to tokens - Done! \\n')\n",
    "\n",
    "    return train_seq_x, valid_seq_x, test_seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    \n",
    "    print(\"load_patentVec-Done! \\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "    \n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def create_bidirectional_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an input layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    \n",
    "    # Add the spatial dropout layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2acd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_test, y_test, model, batch_size, epochs):\n",
    "    history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            shuffle=True)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    # print('Test loss:', scores[0])\n",
    "    # print('Test accuracy:', scores[1])\n",
    "    return scores\n",
    "\n",
    "def predict(model, x_test):\n",
    "    test_classes = model.predict(x_test, verbose=0)\n",
    "    test_classes = np.argmax(test_classes, axis=-1)\n",
    "    # print(test_classes.shape)\n",
    "    return test_classes\n",
    "\n",
    "def mostcommon(array):\n",
    "    '''return the most common value of an array'''\n",
    "    return np.bincount(array).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8268a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(n_learners, epochs_lst, batch_size, sample_ratio=3, number_of_codes=731):\n",
    "\n",
    "#This function creates the base classifiers and calculates their predictions\n",
    "#Then, the function creates the adaboost ensemble classifier by keeping the probabilities from each base classifier\n",
    "#and calculating the final predictions based on alphas and predictions from base classifiers\n",
    "\n",
    "    num_classes = number_of_codes\n",
    "    K = float(num_classes)\n",
    "\n",
    "    #transform one-hot encoded codes to code numbers\n",
    "    train_y_p1_help=np.argmax(train_y_p1,axis = -1)\n",
    "    y_train_old = train_y_p1_help[:]\n",
    "    test_y_p1_help=np.argmax(test_y_p1,axis = -1)\n",
    "    y_test_old = test_y_p1_help[:] # save for error calculation\n",
    "    \n",
    "    #change the names of train_seq_x_p1, test_seq_x_p1, train_y_p1, test_y_p1\n",
    "    y_train=train_y_p1\n",
    "    y_test=test_y_p1\n",
    "    x_train=train_seq_x_p1\n",
    "    x_test=test_seq_x_p1\n",
    "    \n",
    "    n_trains = x_train.shape[0]\n",
    "    n_tests = x_test.shape[0]\n",
    "    \n",
    "    #initislize needed elements\n",
    "    weights = [1.0/n_trains for k in range(n_trains)]\n",
    "    M = sample_ratio*n_trains # >> sample a large (>> m) unweighted set of instance according to p\n",
    "    alphas = []\n",
    "    test_accuracy_records = []\n",
    "    probs = np.zeros((n_tests, num_classes))\n",
    "    probs_2 = np.zeros((n_tests, n_learners), dtype=\"int64\")\n",
    "    \n",
    "    for i in range(n_learners):\n",
    "        \n",
    "        #normalize deviding with sum_weights\n",
    "        sum_weights = sum(weights)\n",
    "        weights = [weight/sum_weights for weight in weights]\n",
    "        \n",
    "        if i ==0:\n",
    "            # use the original dataset\n",
    "            train_picks = np.arange(n_trains)\n",
    "            x_train_i = x_train\n",
    "            y_train_i = y_train\n",
    "        else:\n",
    "            # use the re-weighted train dataset\n",
    "            train_picks = np.random.choice(n_trains, M, weights)\n",
    "            x_train_i = x_train[train_picks, :]\n",
    "            y_train_i = y_train[train_picks, :]\n",
    "\n",
    "        epochs = epochs_lst[i]\n",
    "        \n",
    "        kill_model()\n",
    "        model = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        model, history = train(x_train_i, y_train_i, x_test, y_test, model, batch_size, epochs)\n",
    "\n",
    "        #changes based on prediction results\n",
    "\n",
    "        print(\"model \" + str(i))\n",
    "        predicts = predict(model, x_train_i)    \n",
    "        y_ref = y_train_old[train_picks].reshape((M, ))        \n",
    "        num_error = np.count_nonzero(predicts - y_ref)        \n",
    "        error = float(num_error)/M\n",
    "\n",
    "        alpha = np.log((1 - error)/error) + np.log(K - 1)\n",
    "               \n",
    "        w_changed = np.zeros(n_trains)\n",
    "        for j in range(M):\n",
    "            index = train_picks[j]\n",
    "            if predicts[j] != y_ref[j] and w_changed[index] == 0:\n",
    "                w_changed[index] = 1\n",
    "                weights[index] = weights[index] * np.exp(alpha)\n",
    "\n",
    "        sum_weights = sum(weights)\n",
    "        weights = [weight/sum_weights for weight in weights]\n",
    "\n",
    "        train_picks = np.random.choice(n_trains, M, weights)\n",
    "        x_train_i = x_train[train_picks, :]\n",
    "        y_train_i = y_train[train_picks, :]\n",
    "        \n",
    "        #save the alphas\n",
    "        alphas.append(alpha)\n",
    "        print(\"alpha = \" + str(alpha))\n",
    "\n",
    "        #save the evaluation score\n",
    "        scores = evaluate(model, x_test, y_test)\n",
    "        test_accuracy_records.append(scores[1])        \n",
    "        print(\"accuracy evaluate= \" + str(scores[1]))\n",
    "        \n",
    "        ''' return final_predict based on weighted_vote of all the learners in models\n",
    "        weight is the the accuracy of each learner'''\n",
    "    \n",
    "        pred=model.predict(x_test)\n",
    "        probs = probs + alpha*pred\n",
    "           \n",
    "        ''' return final_predict based on majority vote of all the learners in models'''\n",
    "\n",
    "        probs_2[:, i] = predict(model, x_test) # each column stores one learner's prediction\n",
    "    \n",
    "    print(\"Final scores:\")\n",
    "    #final predict weighted_vote     \n",
    "    final_predict = np.argmax(probs, axis=-1)\n",
    "    errors = np.count_nonzero(final_predict.reshape((n_tests, )) - y_test_old.reshape((n_tests,)))\n",
    "    \n",
    "    #final predict majority vote     \n",
    "    final_predict_2 = np.zeros((n_tests, 1), dtype=\"int64\")\n",
    "    for i in range(n_tests):\n",
    "        final_predict_2[i] = mostcommon(probs_2[i, :])\n",
    "    errors_2 = np.count_nonzero(final_predict_2.reshape((n_tests, )) - y_test_old.reshape((n_tests,)))\n",
    "    \n",
    "    print('Adaboost ensemble - Accuracy based on weighted vote: %f' % ((n_tests - errors)/float(n_tests)))\n",
    "    print('Adaboost ensemble - Accuracy based on majority vote: %f' % ((n_tests - errors_2)/float(n_tests)))\n",
    "\n",
    "    for i in range(n_learners):\n",
    "        print(\"Base classifiers/learners\")\n",
    "        print(\"learner %d (epochs = %d): %0.6f\" % (i, epochs_lst[i], test_accuracy_records[i]))\n",
    "\n",
    "    #Store final predictions\n",
    "    #df=pd.DataFrame(probs)\n",
    "    #df.sort_values(by=0, axis=1, ascending=False)\n",
    "    #df.to_csv(\"F:/PhD/Datasets/ensemble/adaboost/predictions_adaboost_abstract_learners\"+str(n_learners)+\".csv\", header=False, index=False)\n",
    "\n",
    "    # Store rel\n",
    "    #q_rel=y_test_old.reshape((n_tests,))\n",
    "    #df_q_rel=pd.DataFrame(q_rel)\n",
    "    #df_q_rel.to_csv('F:/PhD/Datasets/ensemble/adaboost/qrel_numbers_ada_abstract_5.csv', header=False, index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unnecessary-australian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 60, 300)           52227000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 731)               146931    \n",
      "=================================================================\n",
      "Total params: 52,694,731\n",
      "Trainable params: 467,731\n",
      "Non-trainable params: 52,227,000\n",
      "_________________________________________________________________\n",
      "Train on 432904 samples, validate on 54113 samples\n",
      "Epoch 1/1\n",
      "432904/432904 [==============================] - 450s 1ms/step - loss: 2.5219 - acc: 0.4179 - val_loss: 1.7472 - val_acc: 0.5541\n",
      "model 0\n",
      "alpha = 6.837898882534359\n",
      "54113/54113 [==============================] - 56s 1ms/step\n",
      "accuracy evaluate= 0.5540812743702992\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 60, 300)           52227000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 731)               146931    \n",
      "=================================================================\n",
      "Total params: 52,694,731\n",
      "Trainable params: 467,731\n",
      "Non-trainable params: 52,227,000\n",
      "_________________________________________________________________\n",
      "Train on 432904 samples, validate on 54113 samples\n",
      "Epoch 1/1\n",
      "432904/432904 [==============================] - 454s 1ms/step - loss: 2.5379 - acc: 0.4147 - val_loss: 1.7654 - val_acc: 0.5497\n",
      "model 1\n",
      "alpha = 6.857017567398883\n",
      "54113/54113 [==============================] - 55s 1ms/step\n",
      "accuracy evaluate= 0.549664590763772\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 60, 300)           52227000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 60, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 731)               146931    \n",
      "=================================================================\n",
      "Total params: 52,694,731\n",
      "Trainable params: 467,731\n",
      "Non-trainable params: 52,227,000\n",
      "_________________________________________________________________\n",
      "Train on 432904 samples, validate on 54113 samples\n",
      "Epoch 1/1\n",
      "432904/432904 [==============================] - 476s 1ms/step - loss: 2.4774 - acc: 0.4260 - val_loss: 1.7476 - val_acc: 0.5532\n",
      "model 2\n",
      "alpha = 6.8670371766563045\n",
      "54113/54113 [==============================] - 56s 1ms/step\n",
      "accuracy evaluate= 0.5531757618317225\n",
      "Final scores:\n",
      "Adaboost ensemble - Accuracy based on weighted vote: 0.570990\n",
      "Adaboost ensemble - Accuracy based on majority vote: 0.564910\n",
      "Base classifiers/learners\n",
      "learner 0 (epochs = 1): 0.554081\n",
      "Base classifiers/learners\n",
      "learner 1 (epochs = 1): 0.549665\n",
      "Base classifiers/learners\n",
      "learner 2 (epochs = 1): 0.553176\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 60\n",
    "epochs_lst = [1, 1, 1]\n",
    "batch_size = 128\n",
    "n_learners = [3]#, 5, 7]\n",
    "sample_ratio = 1\n",
    "\n",
    "for i, learners in enumerate(n_learners):\n",
    "    \n",
    "    print(learners)\n",
    "    \n",
    "    '''trainDF_p1 = load_patents(\"F:/PhD/Datasets-Results/clefip/Datasets/I3_dataset_multilabel/abstract.csv\")\n",
    "\n",
    "    onehot_encoded, onehot_encoder=encode_labels(trainDF_p1)    \n",
    "   \n",
    "    number_of_codes=enumarate_codes(onehot_encoded)\n",
    "    \n",
    "    train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=split_dataset(trainDF_p1, onehot_encoded)\n",
    "\n",
    "    token_p1, word_index_p1=tokenize_text(trainDF_p1)\n",
    "\n",
    "    train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "    embeddings_index = load_language_model('F:/PhD/Datasets-Results/embeddings/patent-300.vec')\n",
    "    embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "    #del embeddings_index'''\n",
    "    \n",
    "    adaboost(learners, epochs_lst, batch_size, sample_ratio, number_of_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a1f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
