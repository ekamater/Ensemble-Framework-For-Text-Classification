{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-thompson",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dict=set(stopwords.words(\"english-v2-uspto-sklearn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944ebbf",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b922e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents_text(part, number_of_words, file1):\n",
    "\n",
    "#This function loads the text and the labels from a csv file \n",
    "\n",
    "# part: the textual field that we will use \n",
    "# number_of_words: the first number of words that we will use\n",
    "# file1: the csv file with the dataset containing all textual fields and labels\n",
    "        \n",
    "    trainDF = pd.read_csv(file1, header=None)\n",
    "    \n",
    "    #labels\n",
    "    trainDF=trainDF.rename(columns={8: 'labels'})\n",
    "    \n",
    "    #text\n",
    "    if part==1:  #title\n",
    "        trainDF=trainDF.rename(columns={3: 'text'})\n",
    "    elif part==2:  #header\n",
    "        trainDF=trainDF.rename(columns={4: 'text'})\n",
    "    elif part==3:  #recitals\n",
    "        trainDF=trainDF.rename(columns={5: 'text'})\n",
    "    elif part==4:  #main_body\n",
    "        trainDF=trainDF.rename(columns={6: 'text'})\n",
    "    elif part==5:  #attachments\n",
    "        trainDF=trainDF.rename(columns={7: 'text'})\n",
    "    \n",
    "    #replace the na rows with \"\" otherwise it returns an error\n",
    "    trainDF['text']=trainDF['text'].fillna(\"\")\n",
    "    #delete all symbols except for a-z\n",
    "    trainDF['text']=trainDF['text'].replace('[^a-z]', ' ', regex=True)       \n",
    "    #delete the stopwords\n",
    "    trainDF['text']=trainDF['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_dict)]))\n",
    "    #keep the first #number of words\n",
    "    trainDF['text']=trainDF['text'].str.split().str[0:number_of_words]\n",
    "    trainDF['text']=trainDF['text'].str.join(' ')    \n",
    "    print(\"The data has been loaded! \\n\")\n",
    "      \n",
    "    return trainDF\n",
    "\n",
    "def encode_multilabels(trainDF, trainDF_test):\n",
    "\n",
    "#This function encodes the labels with MultiLabelBinarizer\n",
    "\n",
    "# trainDF: the train dataset  \n",
    "# trainDF_test: the test dataset\n",
    "\n",
    "    #get the labels from the train csv\n",
    "    labels_val=trainDF['labels']\n",
    "    labels_val = labels_val.str.split(',')\n",
    "    \n",
    "    #get the labels from the text csv\n",
    "    labels_val_test=trainDF_test['labels']\n",
    "    labels_val_test = labels_val_test.str.split(',')\n",
    "    \n",
    "    #concat train and text labels\n",
    "    labels_all=pd.concat([labels_val,labels_val_test])\n",
    "    print(\"The number of train labels, test labels and all labels are:\", labels_val.shape[0], labels_val_test.shape[0], labels_all.shape[0])\n",
    "    \n",
    "    multihop_encoder = MultiLabelBinarizer()\n",
    "    multihop_encoded_original = multihop_encoder.fit_transform(labels_all)\n",
    "    \n",
    "    #split the encoded labels into train and test sets\n",
    "    multihop_encoded=multihop_encoded_original[0:labels_val.shape[0], :]\n",
    "    multihop_encoded_test=multihop_encoded_original[labels_val.shape[0]:labels_all.shape[0], :]    \n",
    "    print(\"The labels have been encoded! \\n\")\n",
    "\n",
    "    return multihop_encoder, multihop_encoded, multihop_encoded_test\n",
    "\n",
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "#This function encounters the total number of labels\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of labels: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes\n",
    "\n",
    "def tokenize_text(trainDF):\n",
    "\n",
    "#This function tokenizes the text\n",
    "\n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index\n",
    "\n",
    "def convert_text(number_of_words, token, x):\n",
    "\n",
    "# This function converts the text to sequence of tokens and pad them till maxlen to ensure equal length vectors\n",
    "\n",
    "    maxlen=number_of_words\n",
    "\n",
    "    seq_x = sequence.pad_sequences(token.texts_to_sequences(x), maxlen)\n",
    "    print(\"The text has been converted to tokens! \\n\")\n",
    "\n",
    "    return seq_x\n",
    "\n",
    "def load_language_model(fname):\n",
    "\n",
    "# This function loads the language model\n",
    "\n",
    "    embeddings_dict = {}\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    print(\"The word embeddings have been loaded! \\n\")\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index, x):\n",
    "\n",
    "# This function creates a token-embedding matrix\n",
    "\n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, x))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "\n",
    "def create_bidirectional_lstm_probabilities_kullback(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "# This function creates the classification model based on Bi-LSTM and KL loss\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "    # Add the Embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    # Add the SpatialDropout1D Layer\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "    # Add a Bidirectional Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)                                                                                        \n",
    "    # Add the Output Layer\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "                                                                                            \n",
    "    model.compile(optimizer='Adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "\n",
    "# This function makes the predictions in the test data\n",
    "\n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('The predictions on test data have been calculated! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true                                                                                           \n",
    "                                                                                            \n",
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60b0c5",
   "metadata": {},
   "source": [
    "### Special functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7760cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(multihop_encoded_train):\n",
    "\n",
    "# This function transforms the multilabel encoding into probabilities, e.g. 1 1 0 0 -> 0.5 0.5 0 0  \n",
    "    \n",
    "    a = np.zeros((multihop_encoded_train.shape))\n",
    "\n",
    "    for i in range(len(multihop_encoded_train)):\n",
    "        sum_of_secondary_codes=sum(multihop_encoded_train[i])\n",
    "        #print(sum_of_secondary_codes)\n",
    "\n",
    "        for j in range(len(multihop_encoded_train[i])):\n",
    "            if multihop_encoded_train[i][j]==1:\n",
    "                a[i][j]=float(1/sum_of_secondary_codes)\n",
    "    print('The probabilities have been calculated! \\n')\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91fe598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrix(number_of_test_samples, number_of_codes, predictions_kull, nn, multihop_encoded_test):\n",
    "\n",
    "# This function calculates the precision, recall and f1 score metrics\n",
    "\n",
    "    nn=nn+1\n",
    "    pred_class_kull=np.empty((number_of_test_samples, number_of_codes))\n",
    "    for row in range(number_of_test_samples):\n",
    "        predictions_p1_sort2=np.argsort(predictions_kull[row])[:-nn:-1]\n",
    "        class_number_zeros=np.zeros(number_of_codes)\n",
    "        for class_number in predictions_p1_sort2:\n",
    "            class_number_zeros[class_number]=1\n",
    "\n",
    "        pred_class_kull[row][:]=class_number_zeros\n",
    "    #print(pred_class_kull.shape)                      \n",
    "    print(metrics.precision_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.recall_score(multihop_encoded_test, pred_class_kull, average='micro')*100)\n",
    "    print(metrics.f1_score(multihop_encoded_test, pred_class_kull, average='micro')*100)                    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18919c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The number of train labels, test labels and all labels are: 45000 6000 51000\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  4193 \n",
      "\n",
      "Number of unique words: 12388 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          3716700   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4193)              842793    \n",
      "=================================================================\n",
      "Total params: 4,880,293\n",
      "Trainable params: 1,163,593\n",
      "Non-trainable params: 3,716,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 74s 2ms/step - loss: 4.5720 - acc: 0.0692\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.1663 - acc: 0.1441\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.5426 - acc: 0.1580\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.2379 - acc: 0.1622\n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.0424 - acc: 0.1710\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.9056 - acc: 0.1739\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.8013 - acc: 0.1802\n",
      "Epoch 8/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.7193 - acc: 0.1814\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.6524 - acc: 0.1828\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5964 - acc: 0.1843\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.5502 - acc: 0.1855\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5071 - acc: 0.1896\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4735 - acc: 0.1894\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4413 - acc: 0.1915\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4164 - acc: 0.1898\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3869 - acc: 0.1923\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3647 - acc: 0.1916\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3446 - acc: 0.1918\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3263 - acc: 0.1931\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.3089 - acc: 0.1953\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2942 - acc: 0.1957\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2790 - acc: 0.1954: 3s - loss: 1\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.2656 - acc: 0.1955\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2535 - acc: 0.1947\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2423 - acc: 0.1947\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2313 - acc: 0.1943\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2209 - acc: 0.1956: 3s - loss: 1.22\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.2116 - acc: 0.1980\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2017 - acc: 0.1966\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1941 - acc: 0.1988: 1s - loss: 1.1937 - acc\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "(6000, 4193)\n",
      "85.1\n",
      "16.8203979443932\n",
      "28.08889866872043\n",
      "calculate_metrix-Done! \n",
      "\n",
      "(6000, 4193)\n",
      "61.629999999999995\n",
      "60.90723415469759\n",
      "61.26648551925244\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The number of train labels, test labels and all labels are: 45000 6000 51000\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  4193 \n",
      "\n",
      "Number of unique words: 12424 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          3727500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4193)              842793    \n",
      "=================================================================\n",
      "Total params: 4,891,093\n",
      "Trainable params: 1,163,593\n",
      "Non-trainable params: 3,727,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 4.8909 - acc: 0.0348\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 4.1946 - acc: 0.0760\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.3455 - acc: 0.1411\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.7611 - acc: 0.1552: 5s \n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.4303 - acc: 0.1598\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.2162 - acc: 0.1624\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.0635 - acc: 0.1679\n",
      "Epoch 8/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.9455 - acc: 0.1704\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.8512 - acc: 0.1698\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.7765 - acc: 0.1752: 3s - loss: 1.77\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.7121 - acc: 0.1786\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.6602 - acc: 0.1766\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.6133 - acc: 0.1824\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5722 - acc: 0.1844\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5402 - acc: 0.1846\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5065 - acc: 0.1862\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4776 - acc: 0.1854\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4555 - acc: 0.1842\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4296 - acc: 0.1847\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4102 - acc: 0.1888\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3926 - acc: 0.1882\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3749 - acc: 0.1909: 4s - lo\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3589 - acc: 0.1906\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3433 - acc: 0.1905\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3310 - acc: 0.1948\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3195 - acc: 0.1955\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3076 - acc: 0.1906\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2949 - acc: 0.1933\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2862 - acc: 0.1926\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2746 - acc: 0.1935\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "(6000, 4193)\n",
      "85.15\n",
      "16.830280669389907\n",
      "28.105402134448237\n",
      "calculate_metrix-Done! \n",
      "\n",
      "(6000, 4193)\n",
      "61.096666666666664\n",
      "60.38015548820661\n",
      "60.736297965405264\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The number of train labels, test labels and all labels are: 45000 6000 51000\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  4193 \n",
      "\n",
      "Number of unique words: 30084 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          9025500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4193)              842793    \n",
      "=================================================================\n",
      "Total params: 10,189,093\n",
      "Trainable params: 1,163,593\n",
      "Non-trainable params: 9,025,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 74s 2ms/step - loss: 4.7068 - acc: 0.0583\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.4262 - acc: 0.1144\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.7638 - acc: 0.1381\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.4174 - acc: 0.1500: 3s - loss: 2.42\n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.1938 - acc: 0.1600\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.0328 - acc: 0.1656\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.9065 - acc: 0.1689\n",
      "Epoch 8/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.8004 - acc: 0.1697\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.7103 - acc: 0.1754\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.6332 - acc: 0.1786\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.5704 - acc: 0.1814\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.5121 - acc: 0.1825\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.4638 - acc: 0.1847\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.4177 - acc: 0.1854\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3809 - acc: 0.1881\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3458 - acc: 0.1885\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.3142 - acc: 0.1908\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.2887 - acc: 0.1906\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2598 - acc: 0.1930\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2351 - acc: 0.1932\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.2160 - acc: 0.1949: 4s - lo\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 1.1954 - acc: 0.1914\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.1792 - acc: 0.1951\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1615 - acc: 0.1976\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1440 - acc: 0.1959\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.1302 - acc: 0.1939\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1170 - acc: 0.1954\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.1024 - acc: 0.1989\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.0918 - acc: 0.1977\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.0805 - acc: 0.1990\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "(6000, 4193)\n",
      "82.3\n",
      "16.266965344577677\n",
      "27.164704587963467\n",
      "calculate_metrix-Done! \n",
      "\n",
      "(6000, 4193)\n",
      "60.25666666666667\n",
      "59.55000658848333\n",
      "59.90125256809596\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The number of train labels, test labels and all labels are: 45000 6000 51000\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  4193 \n",
      "\n",
      "Number of unique words: 30084 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 300)          9025500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4193)              842793    \n",
      "=================================================================\n",
      "Total params: 10,189,093\n",
      "Trainable params: 1,163,593\n",
      "Non-trainable params: 9,025,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 4.5966 - acc: 0.0585\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.4180 - acc: 0.1125\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.7894 - acc: 0.1320\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.4407 - acc: 0.1459\n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.2077 - acc: 0.1588\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.0452 - acc: 0.1632\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.9161 - acc: 0.1685\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.8105 - acc: 0.1715: 3s - loss: 1.80\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.7198 - acc: 0.1740\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.6421 - acc: 0.1772\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.5737 - acc: 0.1798\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.5186 - acc: 0.1813\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.4672 - acc: 0.1835\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.4237 - acc: 0.1845: 1s - loss: 1.4249 - a\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3836 - acc: 0.1859\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3489 - acc: 0.1877: 4s - lo\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.3182 - acc: 0.1918\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2891 - acc: 0.1910\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2623 - acc: 0.1906\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2420 - acc: 0.1920\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.2196 - acc: 0.1931\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.1995 - acc: 0.1914: 9s - lo - ETA: 4s - loss:\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1813 - acc: 0.1947\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.1633 - acc: 0.1954\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1490 - acc: 0.1958\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1326 - acc: 0.1984\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1222 - acc: 0.1983\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.1063 - acc: 0.1967\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 1.0968 - acc: 0.1972\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 1.0844 - acc: 0.1992\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "(6000, 4193)\n",
      "82.23333333333333\n",
      "16.253788377915406\n",
      "27.14269996699307\n",
      "calculate_metrix-Done! \n",
      "\n",
      "(6000, 4193)\n",
      "60.196666666666665\n",
      "59.4907102385031\n",
      "59.84160646828817\n",
      "calculate_metrix-Done! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The data has been loaded! \n",
      "\n",
      "The number of train labels, test labels and all labels are: 45000 6000 51000\n",
      "The labels have been encoded! \n",
      "\n",
      "Number of labels:  4193 \n",
      "\n",
      "Number of unique words: 48316 \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The text has been converted to tokens! \n",
      "\n",
      "The probabilities have been calculated! \n",
      "\n",
      "The word embeddings has been loaded! \n",
      "\n",
      "No model to clear \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 300)          14495100  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4193)              842793    \n",
      "=================================================================\n",
      "Total params: 15,658,693\n",
      "Trainable params: 1,163,593\n",
      "Non-trainable params: 14,495,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 4.8562 - acc: 0.0483\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 4.1677 - acc: 0.0821\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.8076 - acc: 0.1010\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.5756 - acc: 0.1047\n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.4059 - acc: 0.1060\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 3.2796 - acc: 0.1076\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.1778 - acc: 0.1090\n",
      "Epoch 8/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.0938 - acc: 0.1102\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 3.0217 - acc: 0.1134\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.9585 - acc: 0.1149\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.9015 - acc: 0.1148\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.8503 - acc: 0.1162\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.8063 - acc: 0.1181\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.7625 - acc: 0.1215\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.7253 - acc: 0.1207\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.6902 - acc: 0.1215\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.6569 - acc: 0.1233\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.6257 - acc: 0.1221\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.5975 - acc: 0.1252\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.5713 - acc: 0.1249: 4s - loss:\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.5446 - acc: 0.1264\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.5228 - acc: 0.1273\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.5002 - acc: 0.1286\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 73s 2ms/step - loss: 2.4795 - acc: 0.1284\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.4595 - acc: 0.1302\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.4400 - acc: 0.1312\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.4233 - acc: 0.1290\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 71s 2ms/step - loss: 2.4087 - acc: 0.1288\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.3931 - acc: 0.1306: 2s - loss: 2.3925 -\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 72s 2ms/step - loss: 2.3773 - acc: 0.1312\n",
      "The predictions on test data have been calculated! \n",
      "\n",
      "(6000, 4193)\n",
      "53.31666666666667\n",
      "10.538279088153907\n",
      "17.598195621080425\n",
      "calculate_metrix-Done! \n",
      "\n",
      "(6000, 4193)\n",
      "38.376666666666665\n",
      "37.92660429569113\n",
      "38.150308171515675\n",
      "calculate_metrix-Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_words = [100]\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "parts=[1, 2, 3, 4, 5] #1: title, 2: header, 3: recitals, 4: main_body, 5: attachments\n",
    "\n",
    "for i, words in enumerate(number_of_words):\n",
    "    for k, part in enumerate(parts):\n",
    "\n",
    "        train_filename=\"/datasets/eurlex57k_train_multilabel.csv\"\n",
    "        test_filename=\"/datasets/eurlex57k_test_multilabel.csv\"\n",
    "\n",
    "        #load train data  \n",
    "        train_DF=load_patents_text(part, words, train_filename)\n",
    "        #load test data\n",
    "        test_DF=load_patents_text(part, words, test_filename)\n",
    "\n",
    "        multihop_encoder, multihop_encoded_train, multihop_encoded_test=encode_multilabels(train_DF, test_DF)\n",
    "        number_of_codes=enumarate_codes(multihop_encoded_train)  \n",
    "        token_p1, word_index_p1=tokenize_text(train_DF.append(test_DF))\n",
    "\n",
    "        train_seq_x_p1 =convert_text(words, token_p1, train_DF['text'])    \n",
    "        test_seq_x_p1 =convert_text(words, token_p1, test_DF['text'])\n",
    "\n",
    "        a=calculate_probabilities(multihop_encoded_train)\n",
    "\n",
    "        # load the glove language model\n",
    "        if part==1:\n",
    "            embeddings_index = load_language_model('/embeddings/glove.6B.300d.txt')\n",
    "        embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1, 300)\n",
    "\n",
    "        kill_model()\n",
    "        classifier = create_bidirectional_lstm_probabilities_kullback(words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "        history=classifier.fit(train_seq_x_p1, a, epochs=epochs, batch_size=batch_size, verbose=1)           \n",
    "        \n",
    "        #Save the trained classifier\n",
    "        #classifier.save(\"eurlex_part\"+str(part)+\"_30epoch\")\n",
    "\n",
    "        predictions_kull, prediction_kull, y_true_kull=make_predictions(test_seq_x_p1, multihop_encoded_test, classifier)\n",
    "        \n",
    "        number_of_test_data=np.shape(test_seq_x_p1)\n",
    "        number_of_test_data=number_of_test_data[0]\n",
    "\n",
    "        print(\"calculate the P@1, R@1 and F1@1\")\n",
    "        calculate_metrix(number_of_test_data, number_of_codes, predictions_kull, 1, multihop_encoded_test)\n",
    "        print(\"calculate the P@5, R@5 and F1@5\")\n",
    "        calculate_metrix(number_of_test_data, number_of_codes, predictions_kull, 5, multihop_encoded_test)\n",
    "\n",
    "        #Save the final predictions\n",
    "        #df=pd.DataFrame(predictions_kull)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #file_name=\"eurlex_\"+str(part)+\"_\"+str(words)+\"_30Epoch.csv\"\n",
    "        #df.to_csv(file_name, header=False, index=False)\n",
    "        \n",
    "        #Save qrel\n",
    "        #df=pd.DataFrame(multihop_encoded_test)\n",
    "        #df.sort_values(by=0, axis=1, ascending=False)\n",
    "        #file_name=\"eurlex_multihop_encoded_test.csv\"\n",
    "        #df.to_csv(file_name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c65d60",
   "metadata": {},
   "source": [
    "### Load the stored predictions and qrel and create the ensemble of above classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1=pd.read_csv('eurlex_1_100_30Epoch.csv',header=None)\n",
    "in2=pd.read_csv('eurlex_2_100_30Epoch.csv',header=None)\n",
    "in3=pd.read_csv('eurlex_3_100_30Epoch.csv',header=None)\n",
    "in4=pd.read_csv('eurlex_4_100_30Epoch.csv',header=None)\n",
    "in5=pd.read_csv('eurlex_5_100_30Epoch.csv',header=None)\n",
    "\n",
    "in1_predictions=in1.to_numpy()\n",
    "in2_predictions=in2.to_numpy()\n",
    "in3_predictions=in3.to_numpy()\n",
    "in4_predictions=in4.to_numpy()\n",
    "in5_predictions=in5.to_numpy()\n",
    "\n",
    "in1_prediction = np.argmax(in1_predictions, axis = -1) \n",
    "in2_prediction = np.argmax(in2_predictions, axis = -1) \n",
    "in3_prediction = np.argmax(in3_predictions, axis = -1) \n",
    "in4_prediction = np.argmax(in4_predictions, axis = -1) \n",
    "in5_prediction = np.argmax(in5_predictions, axis = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779aa8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv('eurlex_multihop_encoded_test.csv',header=None)\n",
    "true=true.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6866dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_test_data_p1=in1_predictions.shape[0] \n",
    "number_of_codes=in1_predictions.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions5(predictions_p1, predictions_p2,predictions_p3, predictions_p4, predictions_p5, number_of_test_data_p1, en):\n",
    "\n",
    "#This functions calculates the averaging of predictions for each label\n",
    "\n",
    "    average_predictions = []\n",
    "    i=0\n",
    "    \n",
    "    for i in range(number_of_test_data_p1):\n",
    "        a=np.mean([predictions_p1[i], predictions_p2[i], predictions_p3[i], predictions_p4[i], predictions_p5[i]], axis=0)\n",
    "        average_predictions.append(a)\n",
    "    \n",
    "    average_predictions_2 = np.array(average_predictions)          \n",
    "    average_prediction = np.argmax(average_predictions, axis = -1) \n",
    "    print('The ensemble predictions have been calculated! \\n')\n",
    "\n",
    "    return average_predictions_2, average_prediction\n",
    "\n",
    "ensembles=5 #the number of base classifiers combined using an avaraging function\n",
    "average_predictions, average_prediction=ensemble_predictions5(in1_predictions, in2_predictions, in3_predictions, in4_predictions, in5_predictions, number_of_test_data_p1, ensembles)\n",
    "\n",
    "#calculate the P@1, R@1 and F1@1 for the ensemble of 5 classifiers\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 1, true)\n",
    "#calculate the P@5, R@5 and F1@5 for the ensemble of 5 classifiers\n",
    "calculate_metrix(number_of_test_data_p1, number_of_codes, average_predictions, 5, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
